Ethan, you're a master's degree student at Mila in Montreal, you have published papers on out of distribution, generalization, and robustness generalization accepted as presentations and spotlight presentations at ICML and NeurIPS. You've recently been thinking about scaling laws, both as an organizer and speaker for the first neural scaling laws workshop in Montreal. You're currently thinking about the monotonic scaling behaviors for downstream and upstream task, like in the GPT-3 paper, and most importantly, people often introduce you as the edgiest person at Mila on Twitter, and that's the reason why you're here today. So thanks, Ethan, for coming on the show and it's a pleasure to have you.
You're also well-known for publicizing some sweatshirt mentioning scale is all you need AGI is coming.
How did those sweatshirts appear?
Maybe you can like explain intuitively for listeners that are not very familiar to what are scaling laws in general.
And to take like concrete examples, like for GPT-3, the upstream task is just predict the next word. What are the downstream tasks?
And yeah, what should we care about upstream and downstream task?
So you don't get more interpretability as you scale your models?
Yeah, when you mention alignment, what's the easiest way for you to define it?
Gotcha, so it is not like a technical definition. It's more a theoretical one.
So would you consider yourself an alignment researcher or more like a deep learning researcher?
What's AGI?
When do you think we'll get AGI?
What's your like 10% and 90% estimate?
I think there's been a week where we got DALL-E 2, Chinchilla, PaLM. Did that like update your models in any way?
So when you mentioned the IMO stuff, I think it was like at problem from maybe 20 years ago, and it was something that you can like do with maybe like two lines of math.
Why is math supposed to have the worst scaling?
I think in the paper, they mentioned that the method would not really scale well because of, and some infinite actions base when trying to think of like actions.
So yeah, I didn't update it. I was like, oh yeah, scaling will be easy for math.
Okay, related to scaling, the paper by DeepMind about the Chinchilla model was the most relevant, right?
Yeah, he said that on the Q&A, right?
Can you like explain the kind of insights from scaling laws between like compute model size, and then like what's called like the Kaplan Scaling law?
That's also what I remember, and I think like the main insight from the first thing you said from the Kaplan law is that like model size is all those matters compared to dataset and for a fixed compute budget.
So compute is the bottleneck now. What about like having huge model?
Does that make you more confident that we'll get like better performance for models quicker?
Yeah, so I'm curious when do you think about like how much are we bottlenecked by data for text?
So how many like orders of magnitude in terms of like parameters does this give us?
I was essentially quoting Jared Kaplan's video.
Yeah, so I think you said that if you remove the duplicates on GitHub, you get some amount of tokens, maybe like 50 billion tokens, 500, I'm not sure. Maybe 50 billion. Don't put me on that.
And yeah, so the tricks will be data augmentation... you're like applying the real things to make your model better, but it's not clear how do you improve performance? So my guess would be you do transfer learning, like you train on like all the different languages.
My guess is also that like, if you get a bunch of like the best programmers in the world to use co-pilot and then you get like feedback from what they accept, you get higher quality data. You get just like, oh yeah, this work just doesn't work. And so you have like 1 million people using your thing 100 times a day, 1,000 times a day, then that's data for free.
Well, don't you think like having all this telemetric data from GitHub cooperatives is you can use it, right?
Okay. Gotcha. Yeah, I think the other thing they did for improving GPT-3 was just having a bunch of humans rate the answers from GPT-3 and then like that's the paper of instructivity. I think like they had a bit of humans and it kind of improved the robustness or not for business, but alignment of the answer somehow. Like it said less like non-ethical things.
Exactly, yeah. And to go back to YouTube, why is scaling on YouTube interesting? Because there's unlimited data?
And yeah. So for people who were not used to or thinking, what's the task in YouTube?
What's contrastive type thing?
So you'd say that your model takes a video, like all of the videos and views as input?
So he tries to kind of predict correlations between frames in some kind of latent space from a resonance?
And at the end of the day, you want something that is capable of predicting how many frames in lens.
What are the useful downstream tests, like robotics?
And yeah, wouldn't your model be kind of shocked by the real world when you just like show him like YouTube videos all the time and then you trust the robot with like a camera?
Gotcha. So I was mostly thinking about like entropy of language.
So one of the things about the scaling laws papers and the role of scaling laws, there was some different exponents for text.
What do you think is the exponent for video? Would it be like much worse?
So there might be some universal law where scaling goes for all modality and nobody knows why.
Who do you think will win the video prediction race?
So who is able to spend the most money? So would it be USA, China, Russia?
Right, right. But how do you even spend that much money?
From looking at the PaLM paper from Google, they seem pretty clever on how they use their compute.
Right. I think TPU pods or something, they call it.
So it didn't seem like they spent more money than OpenAI. So they tried to be more careful somehow. So my model of like people spending a lot of money is.
So maybe China, but I see Google as being more helpful because of they do it on paper, but maybe I'm wrong.
Yeah, so they have like a different hypothesis. OpenAI is like scale is all that matters, somehow that they're secrets itself and-
You just let scale things and we are going to get better results, and Google is maybe there's more bureaucracy and it's maybe harder to get a massive budget.
They probably know, but then the question is how like private things are and maybe there's stuff we don't really know.
So you've talked to them like privately or is it just, they said online?
So as we're on like gossip, I think like something that was around on the internet, like right when GPT-3 was launched was that Google was like reproduced it in a few months afterwards, but they didn't really talk about it publicly. I'm not sure about what to do with this information.
So we should just like assume all those big companies are just like throwing papers when they're like not relevant anymore when they have like the other paper already?
People want to like keep their advantage, right?
How do you know?
So I think you could just like retrace all Sam Altman tweet and then like you read the next paper like six months after and you're like, oh yeah, he tweeted about that. Like sometimes the tweets like, oh, AI is going to be wild, or oh, neural networks are really capable of understanding. I think you tweeted that like six months ago, like when they discovered GPT-4.
Yeah, neural networks are in some ways slightly conscious.
Yeah, I think people at OpenAI know things we don't know yet. They're all like super hyped. And I think you mentioned as well that at least privately that Microsoft has some deal with OpenAI and so they need to some amount of money before 2024, like.
Okay, so that's in two years?
Right. And so you were not like impressed by PaLM being able to predict to like do logic on airplane things and explain jokes?
Okay, so we need, so if we just have text, it's not enough to have AGI. So if we're a like a perfect Oracle that can like talk like us, but it's not able to do robotic things, then we don't have AGI.
Well, I guess my main like is mostly like coding. So if we get like coding, like Codex or comparative, that gets really good, then everything accelerates and engineers become very productive, and then like.
Thinking about hardware, we're just like humans, Googling things and using.
You mentioned you have like DALL-E, but like for designing chips.
That doesn't make you think of timelines of 10 years or closer.
Oh, so that's your plan. Just get out of compute.
We're kind of on record, but I'm not sure if I'm going to cut this part. So you can say unsafe, it's fine.
You were spotted saying you wanted unsafe AGI the fastest possible.
Or you can just throw nukes everywhere and try to make things slower?
So you want to be, okay, so you want to just like join the winners. Like if you join the skiing team at Google.
Makes sense. So everyone should infiltrate Google.
So I'm kind of happy having a bunch of EA people at OpenAI now, because they're kind of minimizing the text there, but...
Some other people came, right?
I don't know. Richard Ngo.
It's like a team on like predicting the future.
Maybe they use their GPT-5 model and predict things.
They were trying to predict things as well, like forecasting.
But then there's just like doing more governance things and optimal governance and maybe economics.
The guy like Richard Ngo is doing governance there.
Predicting how the future works, I think is in his Twitter bio.
My model is like Sam Altman, as like they have GPT-4. Like they published GPT-3 in 2020. So it's been like two years.
And they've been talking about like in their Q & A about like treacherous results or something like one year ago. So now they must have access to something very crazy and they're just like trying to think like how do we operate with like DALL-E 2 and their GPT-4 they have in private and how they do something without like for him in the world? I don't know. Maybe they're just like trying to predict like how to make the most money with their API or.
Imagine you have access to something that has the same kind of gap between GPT-2 and GPT-3, but like for GPT-4 on like understanding and being general. And you don't want everyone else to copy your work. So you're just going to keep it for yourself for sometime.
That's yeah, that's the best idea ever. So do you have like other gossips besides like Google's? Did you post something on Twitter about people leaving Google?
What's a zillion dollars like?
So did they know about something we don't know? And they're just like get money to replicate what Google does?
But they don't have like the engineering and old infrastructure.
So there's like another game that is in private at Google and they've been scaling huge models for two years. and they're just like,
Starting startups with their knowledge and they're just scaling and we;re just like, like peasants like us talk about papers that are released one year after and then when you turn them out.
So yeah, that makes me less confident about-
I'll take that as a compliment... I started working out. So yeah, Ethan Caballero, what's the meaning of life?
Right. So I've done my Lex Fridman question. Now I'm just basically him.
Maybe we can just go back to like stuff we know more about like your work and because you've been doing some work on scaling.
So like more general, like why are you kind of interested in scaling and like how did you started on doing research on that?
Which paper?
I think in 2019 was also when GPT-2 was introduced.
Right, scaling laws paper is 2020.
And you were already on this killing train since 2017.
And yeah, now you are kind of interested in scaling because it's useful to predict kind of what the whole field of AI is going.
Yeah, if you're a huge company that does a lot of budget, but maybe if you're just a random company, you don't really care about scaling law that much.
You're on an academic budget as far as I know. So how do you manage to do experiments in scaling?
Yeah, so if we go back for a minute on like your work in alignment, how do you think your work on scaling or generalization like kind of fits with the alignment problem?
There's a difference between not improving and inverse scaling. Inverse scaling goes badly, right?
Do we have benchmarks for controllability or?
And kinda to summarize your take, if we were able to just scale everything well and not have this inverse scaling problem, we would get like interpretability and controllability and everything else by just like good scaling of our models. And so we'd get like alignment kind of by defaults for free?
So there's no like test lost on this deception. We don't know for sure how to measure and have a clear benchmark from this.
Knowing that we don't know yet, like all the different benchmarks and metrics for misalignment, don't you think that your work on scaling can be bad because you're actually like speeding up timelines?
I get you but like on a differential progress mindset, like Jared Kaplan or someone else will come up with those functional forms without your work.
Right, so you're kind of publishing papers when you're in advance because those companies are not publishing their results?
And you were just like drawing infohazard by publishing those laws?
Another kind of view on that is that if people do impressive deploying or ML board and they're also interested in alignment, it's still a good thing. Like let's take even through AI. Even if they open source their model because they did something impressive and they talk openly about alignment under Discord and gets like a lot of people that are very smart, interested in alignment. So if you publish something and you become like a famous researcher, something in two years and you talk about alignment in two years, then it's fine.
So if we go back to thinking about predicting future timelines and kind of scaling, I've read somewhere that you think that in the next few years, we might get billion or trillion times of more compute, like 12 orders of magnitude more.
So you need a lot of bandwidths to train models because of the prioritization thing, but you only need flops to simulate on different things at the same time?
And what was that kind of simulator you were talking about, the Kenley?
I haven't seen that.
Okay, so maybe one kind of last open-ended question. On a scale from Paul Christiano, Eliezer Yudkowsky, Sam Altman, how optimistic are you?
Right. Yeah. So you are less optimistic than Sam Altman?
So do you have like one less monologue or sentence to say about why scaling is a solution for all alignment problems?
Awesome, so we get the good version, as last sentence, and that's our conclusion. Thanks Ethan for being on the show.
Blake, you're an assistant professor in the Montreal Neurological Institute in the School of Computer Science at McGill University. You are also a core faculty member at MiLA. We've had other people from MiLA on the podcast, including Ethan Caballero and Sonia Joseph, which I believe you have supervised or will supervise in the future.
When people ask on Twitter, "Who's the edgiest person at MiLA?" Your name got actually more likes than Ethan. So hopefully this podcast will help re-establish the truth. Thanks Blake for coming on the show.
Yesterday, I did a post on Twitter about a political compass related to Artificial General Intelligence or AGI. And you ended up being placed on the "AGI Optimist"/"AGI Not Now" Camp. I'm curious, how do you feel about your position in the starts? Would you rather be somewhere else instead?
Would you say you're closer to Yann LeCun than Gary Marcus or?
Got you. And I think Yann LeCun's point is that there is no such thing as AGI because it's impossible to build something truly general across all domains.
So we will have system that do those separately, but not at the same time?
Why do you think that?
Well, if you look at people, I think some people are more considered polymaths and are somehow good at a bunch of things. But, if you had more time or a bigger brain... Computers are not limited by longevity and memory. So you could just have different programs for Michael Jordan and Blake Richards in different part of your brain. And I think that's one of the criticism of Gato. Maybe the neurons are just for a personal task and you're just choosing which ones to activate for a specific task.
Right. So basically you're saying that having some same architecture that does a bunch of different tasks is super hard or maybe impossible-
... even given the no free lunch theorem. And what will happen is we will have something like possibly Gato that can be switched to different modes depending on the task. Or humans that do like specialized things or narrow AI doing specialized things, but never like one agent doing a bunch of different things.
Right. I think there are clear definitions for intelligence, such as the ability to achieve one's goal? I think that's what Nick Bostrom uses in his book. Intelligence is like... Yeah, if you have a goal of trading stocks and, it is very good at this goal, then you can call it intelligence. And I think the more general case is, if you're able to surpass the ability to achieve one's goals in a bunch of different goals, then in some sense you're smarter. You're more intelligent than humans. You're more able to achieve your goals. And especially for goals that are economically valuable, that will be what we care about, because if you have AIs that automate 80% of our economy, that's more important than 90% of people on Twitter thinking that if we have achieved AI, because then you'll have maybe 90% of people that will be out of job. So, I think that's... Maybe what will happen first is people getting out of job and trying to understand why they're out of job.
Right. So you were saying it's not a practical definition because you could have a bunch of different goals and you could have emerging properties in cells that show intelligence without having any intent?
Right. So I think what you're saying is maximizing reward over time is a interesting metric because that's like the kind of his end goal
Yeah. Terminal goal, we could say, at the end of his life if he maximizes reward... he will be happy. But then in the midst of doing that, he might learn how to open doors and maybe build things. And those will be the instrumental goals, the instrumental tasks he will learn along the way to do his end goal. And those things will be more closer to what humans do. And some of them will be necessary. Like, I don't know... Let's say manufacturing new servers or implementing AI software or all those things and the things that they will converge eventually like gather more resources or build more computers.
Those are convergent instrumental goals. And, I guess, the main argument is that even if you can judge that the end goal that the human programmer put in the system are stupid in a moral sense or aesthetically, it might in the meantime do pretty scary thing pretty well and build a bunch of gigantic computers everywhere. So, if it wants to trade stuff on the stock market and invest in Bitcoin, Ethereum and end up making the market crash like what happened recently, you might not consider it intelligent, but it might have a big impact on the economy.
Do you have maybe better definition for intelligence or other concepts that you find more useful? I know that Yann LeCun prefers to use human level AI instead.
Do you have other stuff like to talk about, with your students or research?
So you would try to distinguish different kinds of intelligence and benchmark those?
One type of intelligence that people care about is writing code. A bunch of developers are happy with GitHub Copilot. And the nice thing with code is that you can also code AI. Do you think we'll ever have AI writing AI codes and at the end self-improving AI or is it too far-fetched for you?
I guess the argument goes something like, if you put AI very good at writing code and you give it enough time, it might build like a smarter version of himself?
And if we stay on your example and we keep your assumptions that you cannot be good at everything, the AI gets incredibly good at writing code. And then he's good at writing AIs that do specialized things. And it does like one that does trading, one that does politics and could we have some main AI that controls a bunch of other ones and as a group, they're running the world or something?
Right. So the humans will be the one controlling the AI and possibly doing bad things. The AI might just do specialized things and not be agenty or trying to do negative things to the world. And yeah, the problem will be the human biases that we into our AI not the AIs themselves?
So that's the AGI good or an AGI bad acts that, I think, people preferred in the comments, AI alignment hard or AI alignment easy. That's more politically correct. And yeah, on the other axis, so "AI soon"... Sorry, "AGI Soon" or "AGI Not Now", I guess the main proponent of the thesis that AGI will arrive soon say something along the lines of "scale is all you need"?
Which is a joke with "attention is all you need". So one of them was our guest previously Ethan Caballero, and you might supervise him in the future. Do you agree with Ethan on "Scale is all you need" or you think it's just a funny meme?
And do you also agree with the bitter lesson from Richard Sutton that the meta-learning methods are better than the handcrafted methods that people use on their own?
I think what you said though, was that you should focus on methods and algorithms that scale. So in that sense, transformers and diffusion are stuff that scale.
When you do modeling of the brain or Reinforcement Learning in general, you tend to start with two-dimensional grid worlds or mountain car and 2D- sorry, one dimension mountain car, and then the true problems arise when you try to do robotics and you have maybe like 13 different variables in your environment. And I believe... So, you are neuroscientist trying to model things with AI, is that correct?
I remember seeing a question you asked on Twitter on what were the better environments to train your arguments for them to be general. So the environments might need to be embodied, interactive, or open-ended. Why do you think you need those three things embodied, interactive, or open-ended to have general agents?
We need to build AI's human-like or could we have intelligence without those human senses?
In reinforcement learning or robotics, the main goal is to interact with your environment and try to grab objects and interact in the real world. One of the problems that sometimes arise is trying to figure out what gets you reward and what doesn't get you reward. And one problem that happened in deep learning was how do you credit which neurons were capable of giving you the output you wanted, and in RL is what action gives you the reward you wanted at the end. And so the general problem is called as the credit assignment problem. And that's one of the focus I think of your lab, one of the things you're interested right now-
Do you want to just elaborate a little bit on what's the problem and what you care about it, what is difficult and other things?
Yeah. I love your answer. And yeah, temporal difference learning is truly essential in RL at least for the most basic models. And I think even when you consider even more complex algorithms using neural networks, they're still using temporal difference learning. I'm curious what's the answer for backpropagation in the brain? How do you study with neurons responsible for out outputting the correct actions in the brain? What's the like experimental method here?
So for now you mostly build models and write equations and then maybe run, Python scripts or PyTorch code? And you try to see if it maps the data that you have recorded previously or some other team recorded?
So you mentioned burst of action potentials. I don't think everyone is well versed in neuroscience-
Just... Okay. Assuming they are for now. I know some researchers at Deepmind have sometimes modeled phasic dopamine as what would transmit reward prediction error among the brain. So, if I want to have candy and I opened the bag and there's no candy, I will be disappointed. That would be a negative reward prediction error. And so that would be negative dopamine... I don't know. I don't know if that exists.
So we all know what dopamine is or at least if we're interested in addiction, we know that dopamine is the thing that is present in our brain when we scroll Twitter. So what would be burst of action potential and how does that relates to AI or reward learning?
Right. So those bursts, they signal something like backprop and they go through the axons. And so you also have chemical backprop as well?
Is your research on credit assignment in the brain trying to inform credit assignment in AI? Or is it mostly to understand the mechanism in the brain?
How do you benchmark or test those algorithms? Do you use, I don't know, Mujoco environments over a long periods of time and try to walk with Hopper things or do you even go on the Montezuma revenge or DOTA2 route and try to do very, very long credit assignments? What's the method here?
Can you grasp the orange without the tools or you need to...
Were you happy with how OpenAI solved with quotes Dota 2 or Uber solved Montezuma's revenge or do you think the problems are still open-ended and it's only brute force for now?
I think they were just doing PPO.
So there were some tricks that made it work in Montezuma's revenge?
I think for Dota 2, what people were saying is that the game is so complex that you would need to have very smart agents to play at a competitive level and coordinate among a group of five because there's so much communication between humans. They know how to communicate between teammates, and you were like, "No, you could never do it with brute force." And the cooperation between teammates emerged from just massive amounts of training.
And I think that's possibly what people were saying with chess a long time ago. They were like, "To solve chess, you need this kind of intelligence that humans have, decision making and so on." And at the time it was also mostly brute force. So yeah, I'm curious if you think that the things we think as intelligence in general cannot just be solved with scale and we just go back to our discussion before and yeah. So, more generally, would there be some event or discovery that could make you think like, "Scale is all you need." We're going to have smart agents that do a bunch of things and make you think more about existential risk. Would you consider something that can change your mind on this?
I think that's an awesome conclusion for the podcast, living us with a bet on the future. It was a pleasure to talk with someone so well informed in neuroscience, AI, reinforcement learning. And yeah, I hope we talk soon and see you on Twitter.
Do you just want to plug your Twitter account or something so people can-
... see.
Yeah,I love it.
This is the end of the episode. My current goal for the channel is to become number one channel called The Inside View. So if you've enjoyed this podcast, I would suggest living a rating on Spotify, Google Podcast, Apple Podcast, or subscribing on YouTube so that when we google "The Inside View Podcast", the first one is not "An Inside View" About Sports, but "The Inside View" about the future of humanity and artificial intelligence. I wish you a great day and see you in the next episode.
Raphael. You're a Presidential Scholar in Society and Neuroscience at Columbia University. You have previously completed a PhD in Philosophy in Oxford. You're interested in the philosophy of mind, cohesive science and artificial intelligence. The past couple of years you've been discussing at length the recent progress in AI, especially with popular twitter threads on GPT-3, DALL-E 2, and AGI. You've recently been highly critical of a thesis you call, scaling maximalism and that's one of the reason you're here today. Thanks Raphael for coming on the show.
Before we go into too much details about the recent progress in AI. think we should start like all good philosophers by some definitions that will help ground the discussion. One of the most heavily loaded term I use on the podcast is AGI, for artificial general intelligence. Could you maybe give us a couple of definitions of AGI that people use in practice?
So you would get some general animal intelligence and then general human intelligence. And then after that you would get superintelligence.
Yeah, that's why I think the definition of superintelligence from 's book was the same name is useful because it defines it as having orders of magnitude, more intelligence than humans in economically viable tasks. So the whole of what humans would consider useful for our economy, machines could do it orders of magnitude, like 10 times faster or better. And how we define intelligence is that, in a very practical way, where intelligence is just the ability to achieve your goal. So if the goal is designing chips or driving a car, if you're 10 times safer in driving the car, or you produce 10 times more chips per hour, then you're more intelligent in that sense. And so your definition of superhuman intelligence with different levels of generality, I would say that it's interesting, but in practice we would not really care because it would be out of our scope. And I think when we think about the future, we care about human level and we care when it's smarter than us in a way that we're not really useful anymore. What do you make off those definitions?
Yeah. So I think you could always find some specific states of our economy or world where we're at nuclear war and there's only this and this that is useful. And okay so the definition adapts to those situations, but at the end of the day, what counts is what fraction of the GDP is produced by humans versus machines. And if I don't know, 20% of our economy is automated. Then that gives us roughly some sense on where we are in terms of automation. And when we get to 98%. So for me, it's more like a continuum, right. And before we reach a hundred percent, we might get to 95%. And the only people doing useful work would be the AI researchers. And I think it's interesting to come back to what you said about behavioral. Sorry about my English. Yeah. How would you define behavioral intelligence. What's the difference in philosophy between behaviors and knowledge, or understanding, if any?
In the context of a Chinese room experiment. Do you think the system is actually intelligent or not?
When you have some human in a room and he has a manual to translate Chinese. And for people who were not familiar, you get an entry some input to get some English text, and then you have a big dictionary when you can map English words to Chinese words, with some very specific rules. And at the end it outputs translated text.
Yeah. I think my point was mostly that if you consider the whole system, an example of a system that could have a behavior that is useful, like translating English to Chinese, without having some understanding. And I guess what you meant was replacing this rule book with a transformer. When you get a large language model that translates effectively English to Chinese without actually having an understanding of the world. And it wouldn't really be able to explain thoroughly what is a cup of water and why humans drink water, but it's still able to translate all the sentences related to cup of waters.
I think that's somewhat similar, even if the technology is quite different from the 1960s to today. And you're mostly concerned about understanding, and at the end of the day, we might end up with AI's that are similar to transformers and GPT-3, and that could have an impact in the world without having a perfect understanding of everything they're doing, but still have a massive economic impact. And yeah, perhaps if we include in understanding, like having some human experience of life, maybe they will never have a human experience or a consciousness or anything related, but they will still be able to transform the world as we know it.
I think most people in the ML community would agree that we are very far from ELIZA right now and they have some competence and they're still beneath semanting understanding from humans. So, I don't think your take is super controversial.
Sure. And, the previous guest on our podcast was Ethan Caballero, he was on an extreme that you called scaling maximum that we will talk about later. Just to go back to your analogy with the octopus. I like it because the octopus has eight arms, a transformer with eight attention heads or something. And it's really close to modeling the understanding of a large language model. And in some sense, what you said about communication intent. So to have communication intent, you need to say something because you want an impact in the world. You want a consequence. I say something to you because I want you to get the information and transmitting and maybe do something about it. But there's also the way in which when you start a word and then you start a sentence or a paragraph, you try to push the essay in a certain direction.
And if you're only predicting the next token without having a clear plan of what you're going to say in three pages, then you're doing this Stochastic parrot thing where you're saying something coherent without really aiming for a particular direction in your wording. And I think one of the first time I've seen your name was two years ago when you did some relatively impressive text generation with GPT-3 where you gave as input text from famous philosophers, so GPT-3 had to answer those and you maybe generated multiple times certain paragraphs to have the right ones. But if you want to just talk a little bit about it, I would be curious to hear take on this.
It is so long ago, two years ago.
I'm curious, do you see a future for philosophy professors in correcting essays? How do you grade essays when in 2025, when people will have access to something that doesn't generate three paragraphs, but maybe entire pages or multiple pages? How will you keep up with those advances?
When will you be out of job?
And at the time you will have brain computer interfaces dictating GPT-5 output in your brain directly.
I think it's always a question of out of distribution generalization. So the chameleon you describe is able to fit in high dimensional space between a couple of data points, and you can transition between one data point to another in a smooth way, because the chameleon changes his color. And at that moment you're doing high dimensional interpolation which is a hard problem. But you're still inside your training data. And I think the most robust or difficult definition of intelligence or general intelligence would be to be able to generalize to out of distribution. You're seeing something from an alien life form, and then you get the problem of "no free lunch theorem", "it's impossible to generalize to anything at all". But I think with Gato or those RL tasks that have been going on in the past few weeks, you get some generalization and you're able to do very well on a bunch of different tasks.
I think one way to measure if you're really, I don't know, a monkey, I think a monkey is able to learn tasks very well. If you're not a stochastic chameleon, but an actual monkey or chimpanzee, able to learn new tasks, then you'd be able to learn Zero-shot or Few-shot new task, right? And what GPT-3 showed was that it was able to be fine tuned to completely new domains and get good performance. And on top of that do very well few-shot learning on some arithmetic task or something else. So, I think we're getting closer to animal intelligence and human intelligence in that regard.
Right. So I think what you're referring to is that you're not changing something in the memory of a language model when you're doing few-shot Learning, but you're rather activating certain parts of the weights by starting by a prompt that says task number one is this, task number two is this, please do task number three. So you're accessing something by prompt-engineering. So I agree with that's claim. And I agree also that, from what I've seen, there were not a lot of Transfer Learning happening in Gato. I think what it showed was mostly that you could have a single architecture that did a bunch of different things, with, I reckon only something very close to transformer and with the same process of optimization would be for language, for vision, for robotic task. But yeah, maybe that's not truly relevant for out of distribution generalization.
I guess it depends on the timelines you ask, right? Is this something possible for humans to do, to build or if it's possible this century or this decade. But to come back to just the Steve Wozniak definition, I don't fully understand what's the setting. Can you train your model, waking up in different rooms and brewing coffee in different rooms or different apartments, and then it encounters a new apartment and has do this? Or it has to learn how to do it by never interacting with a flat.
Right. So I think there's a question of dexterity then there's the vision and be able to classify images and see coffee makers. I think that's not the hardest part, but maybe in this setting, you wake up in a black room and need to find the lights first and open the doors. And I guess the true question is the same as for self driving cars is, would you be able to do it reliably? So in a thousand different apartments, what is the likelihood of it actually turning on the coffee maker and brewing coffee, right?
I guess doing it for, I don't know, 50% chance, or even 10% chance I think is not out of our range today, but doing it reliably, like a human would like every day, that's way more difficult. So you mentioned a lot François Chollet and Yann LeCun, who are French and also a bit of contrarian to the AGI takes. I just want to go back to one of the things you said at the real beginning, where for Yann LeCun there is no Artificial General Intelligence, because it's impossible to generalize completely to any input.
Where do you fall in those Twitter persona, like Chollet, maybe Gary Marcus and LeCun... Even our previous guest, Ethan Caballero, was more bullish on AI progress. Where do you see yourself in this spectrum?
Right. Yeah. I brought up the French thing because we are two French people speaking in English about French researchers and it was funny and yeah, about the Chollet versus Gary Marcus distinction. I think they're both maybe saying something similar that deep learning has some limitations, but they're pointing at different solutions where Chollet has a focus on generalization. And you mentioned his paper on how to measure intelligence with his new data set. And I think Gary Marcus is more focused on symbolic approachs for AI.
So you mentioned multiple times scaling maximalism and we've never defined it precisely. So can you maybe define this position or give the best steelman of that position?
And I think when we mentioned scaling, there are a few things to consider. There's the scaling of compute. So you throw more flops, and/or more duration at your system, then there's scaling the size of your models. And then there's maybe like some architectural tricks to make it work for different data or to scale more effectively. And when we say just scaling, we sometimes dismiss those threads or things people need to consider to make it work, gathering more data, different type of data, maybe multi-model that would transfer. And I think you could say that scale is all you need is a meme that dismisses all those electrics, but points at, we will not need more innovation. We will not need completely different architecture. And maybe a transformer is possibly what we need. And maybe those people only give a 50% chance of this being true. But so far we haven't seen anything, any evidence that would go to like the opposite way.
So I think Richard Sutton said that most innovation would be in the scaling or how to scale models and the tricks from researchers trying to get a few percent increase in benchmarks from some mathematical tricks or small architectural changes will be dismissed after a few years. And what will count will be the meta models, the model that will be able to generalize from more data. And his argument was pretty close to saying scale is all researchers actually need and not innovation.
To summarize your summary of Sutton's view, you're saying that you advocate for less feature engineering in how we try to train our models and give more data to our models and they build the feature themselves, and they're able to meta learn tasks instead of having humans preparing the task over. Is that a correct summarization?
And so I think a specific claim about scale is what we need is, "scale is all you need but for feature engineering or RL". And then there will be other... scale is all you need" is a more general claim and it applies to NLP and other subfields of AI.
I would nuance it by saying, in reinforcement learning, you want to train a model to play specific games or a set of games or tasks. And the feature engineering I was referring to was mostly maybe like hyperparameter search and try to change your algorithm to fit a certain environment so that your model would learn how to play a certain game. And if you're able to have something that plays like a bunch of different games and meta learns how to learn games, then you wouldn't need to change your hyperparameters, or change your training algorithm. It'll just be able to learn effectively by himself because you've seen a bunch of different things. I think that's a better description, probably.
Right. So meta learning is in practice more useful than researcher tricks from the 2010's or 2000's. I think that's what he meant.
Because we've talked about some thought experiments and you're a philosopher, I wanted to ask you this question: imagine Raphaël Millière from 2025, is now convinced that scale is all you need, or maybe a weaker: version. And he comes right now and he says: "Hey, Raphaël, this is what happened. Now I'm convinced." What would be: something likely that he would tell you?
So I think there are different ways a model could learn what's happening better and better in the early 2010s was supervised learning. We got from good to almost perfect classification of ImageNet by just having deeper models. And so, you label a bunch of data and then you fit it to your model and people were disagreeing like "Oh, a baby can just see a couple of examples and doesn't need like a million labeled images to know what's a cat or dog."
And then right now I think that's something Ilia from OpenAI said recently is that it's crazy that now unsupervised learning just works and wasn't working for like decades and now it just works, for no reason at all. And people are unsurprised. They're just like, "Oh yeah, it works." And you're just able to train your model on billions of tokens and that's crazier than what was happening before. Now you're not labeling everything. The fact that we have something that can just like predict the next token and have good performance and a bunch of other tasks, or the GPT-3 downstream tasks, arithmetic, and a bunch of different things. It's crazy in itself, right?
What you're saying is that's not how humans learn, right? But this is still thinking about only unsupervised learning and/or supervised learning, but then there's like reinforcement learning that is like closer to what humans do. And there's a massive progress in RL. And people are showing that you can mix all those different learnings to get something even better. So I think when we were just saying that we give it a bunch of examples or just predicting the next token is not like what humans do. It's kind of dismissing what we could do in the future. Or like what is actually happening in RL research.
Oh, sorry.
What do you mean by continual learning?
You would want something that is connected to a real world stream of data, and update its weights in real time after each example. And the online learning part is, instead of learning in an offline fashion, it's trained by directly accessing a stream of realtime data.
So you've talked a bunch about the different benchmarks that could convince you today that scale is all you need is a thing. And you started with, things are kind of too difficult, which is, of course, if you achieve AGI, then you would've been convinced after seeing the evidence. And when you say "full self-driving," to me, full self-driving is in some sense AI complete or AGI complete. You would need to have the AI to achieve full self-driving, but okay, I get you.
I think people from the "Scale is all you need" camp, who were very bullish on the AI, would tell you is that, "The goal posts have moved". And if I were to interview you maybe like five years ago, you might have, maybe not you, but like someone else might have been very impressed by where we are today.
But right now we need like another level to be impressed. I think there was a meme, sorry, a tweet by Miles Brundage from OpenAI, who was saying, "AI researchers from 2025 be like, 'Oh, this model can generate an entire movie, but the plot is pretty bad. And the actor is not really convincing.'"
So, right now we have models that can create realistic images with DALL·E and Imagen. And people are criticizing a specific part about the compositionality or human faces, or a bunch of different things. But we're never going to be fully impressed, because things are kind of moving in a continuum. So, yeah, how do you feel about this whole goal post moving in the field of AI?
Hopefully we can avoid goal post moving in both camps. You started using the word "should," which is a very specific word for a philosopher, where I think... That's where we disagree because... And that reflects what we say "is" about the world. Because I think people concerned with existential risk from AI are looking at AI progress and thinking about what could happen if something is true or not. So if scale is indeed all you need, then maybe there's like, there's a 10% chance. There's a 10% chance, that in 5 or 10 years, we would get something close to AGI.
And maybe then we can think of, like, what would be the impact in the world, and if we come back to the ought question, should we prepare for those risks right now, considering there's a 10% chance of something dramatic happening in 10 years. And maybe what the AI skeptics are saying is... They're probably more in the camp of, like, AI is so difficult that we're trying to make progress. And all those AI researchers are in the same camp of trying to make progress on this hard task.
And when they produce a paper, they're trying to make progress. And what Gary or other skeptics are doing is maybe bringing more nuance and saying, like, "Oh, cool, your results are pretty good, but you're not really generalizing to this." But the goal is always like, "Make progress in AI." And when you see this, you're like, "Okay, cool," so you're being useful because you're pointing out limitations. And then we can eventually make more useful progress.
The other, I guess, I think there's like another camp, who are maybe like the singularists, or the AGI cult, I will say, who think that it's going to be good, or there's a high chance of it being very good for the world and lower risk, or maybe they dismiss the risk. And this camp is very bullish on AI, and we'll see everything as like, "Oh yeah, we're going to get closer to something very good for humanity."
And yeah, I think Gary is maybe skeptic of this view or, or think the thing is very far away. So I think there's a bunch of different criteria and people talking about it. And at least on my part, when I think about current progress, I think about if this is true and scale is all you need, what would it mean for humanity? And should we prepare for it or not?
And I'm not doing motivated reasoning of it. I don't want the thing to happen. I don't want to reach AGI too soon, for me. And I guess another thing you were saying is, about social media and like people talking past each other, and a silent majority of people not saying anything. And you were saying that your takes were not very controversial, and that there was a silent majority of people thinking the same things as you do or something close.
And there's also something close related to existential risk from AI, where if you're like an existing AI researcher at some prestigious lab or university, you cannot actually talk about existential risk, because it's somehow out of the Overton window for AI researchers. And to me, there's also a silent part of the researchers that are concerned about AGI or concerned about existential risk, but cannot really talk about it out loud, because otherwise they would lose their job.
And it's starting to open more and more, and people are talking about it more and more because they're more impressed. Right? But I think there's a silent majority in both camps.
It's not always that simple, right? So, we both agree that, I guess, that there is some risk of catastrophe from climate change in the decades that are coming, right? But we're not actively working in those because we don't have an omnipotence on the outcomes. And so, I think so some people see AGI as climate change probably as something that could impact their future in different degrees. And they might not have a huge impact on this.
And they're just like seeing this from outside. And as an egoistic point of view, they're just like, "Oh, why am I going to risk my job and my reputation, it's only going to change by," I don't know, "0.001% the outcome." And also those teams, so they're a bunch of different labs working on existential risk. So those people that are not expressing themselves on social media, they might also have jobs that are not at Baidu or Microsoft or those things, but at actual labs working on existential risk.
And those labs working on AGI, so maybe like OpenAI or DeepMind, also have safety teams or alignment teams. So it's not like a clear separation between capabilities and safety, it is much more nuanced than that.
You're pointing out activism. So they could be more effective ways or things that people are not doing, which is strong activism against scaling or AGI. And I think those things are being brought up more, but it's not the best way to bring this issue to AI researchers. I think if you want to actually push the field of AI Alignment forward, you want to bring good research to the world and show that those people are not crackpots from the 2000s thinking about something very far fetched, but there are actually knowledgeable researchers trying to make progress in things.
So activism is being brought up, but is not the main concern at the time. And so to just answer your concern about those people working on scaling, you know that there's, again, a continuum between training big models, training large models, and being the one, being Jared Kaplan, publishing like the scaling loss paper at OpenAI.
So there are various ways to work on this, and working on AI makes you a better researcher that could possibly align the things. So like instrumentally, even if you do some research that pushes the AI field forward for 10 years, and after 10 years, you do some, like, five papers on the alignment, like the fraction of the research you do in the first 10 years, it's so minimal compared to the other millions of researchers doing things, that your impact is very minimal.
But at the end, there are so few people doing alignment, that like being an expert in this, in AI, will make a significant impact in the overall research. Right? So if there are like 100 researchers doing alignment, and you're one of them, you're doing a huge impact and learning things early on requires you to interact with AI. And the way to go is, work at Google for, do a PhD at Stanford, or other things, because otherwise you will just be someone writing about far-fetched topics without any AI understanding.
Right. And I think, for an example, on nuclear war, or nuclear risk, for the Manhattan projects, there were probably hundreds of researchers in the US working on this, maybe same amount in Germany. And they didn't really know that the thing would succeed. They were just trying, and they were, and it was very unclear for them how long would it take?
And for the people who actually did nuclear fission for the first time, I think they were quite surprised. And yeah, in for AI, I think that the scale is just so different that instead of being like one in a hundred, you might be like one in a million. So your impact is much less, right? As an individual.
Yeah. To just go back to something you said before about compositionality and the things you've been reacting to more recently on DALL·E and Imagen, could you maybe define what's compositionality for the other firms or maybe like in the context of image generation?
Yeah. So just to be more practical, the examples you gave was of a horse riding an astronaut that's something Dall-E or Imagen are not able of doing. What were other examples of the limitations in practice, what are the images that they were not able to represent?
Do you know how it uses the language model?
Gotcha. Yeah. I agree that...yeah. From the images you have online, you don't have a bunch of a red cube on top of a blue on top of a green one, and it's easy to find counter examples that are very far from the training distribution.
I'm curious about your experience with Dall-E because you've been talking about Dall-E before you had access. And I think in the recent weeks you've gained the API access. So have you updated on how good it is or AI progress in general, just from playing with it and being able to see the results from octopus, I don't know how you call it.
Let's think about this problem step by step.
Isn't that already something, a feature of the Dall-E API? At least on the blog post, they have something where they have a flamingo that you can add it to be, remove it or move it to the right or left.
Right, so you split your sentence with the "and" and then you com combine those embeddings to engineer the image. I think, yeah, as you said, it is probably the general solution is as difficult as solving the understanding of language, because you would need to see in general how in a sentence the different objects relate to each other. And so to split it effectively, it would require a different understanding. 
I'm curious, what do you think would be kind of the new innovation? So imagine when we're in 2024 or even 2023 and Gary Marcus is complaining about something on Twitter. Because for me, Dall-E was not very high resolution, the first one, and then we got Dall-E 2 that couldn't generate texts or yeah. Do you know faces or maybe that's something from the API, not very an AI problem, and then Imagine came along and did something much more photorealistic that could generate text.
And of course there's some problems you mentioned, but, do you think in 2023, we would just work on those compositionality problems one by one, and we would get three objects blue on top of red and top of green, or would it be like something very different? Yeah, I guess there are some long tail problems in solving fully the problem of generating images, but I don't see what it would look like. Would it be just imaging a little bit different or something completely different?
As you said, if you knew what the future would look like, you would be funding as a series B startup in the Silicon valley, not talking on a podcast. Yeah. I think this is an amazing conclusion because it opens a window for what is going to happen next. And, yeah. Thanks for being on the podcast. I hope people will read all your tweets, all the threads on compositionality, Dall-E, GPT-3 because I learned personally a lot from them. Do you want to give a quick shout out to your Twitter account or a website or something?
Yeah, sure. I will definitely join. I will add a link below. I can't wait to see you and Gary disagree on things and make predictions and yeah. See you around.
How did you react when you saw this meme? Did you find it somehow represent your views?
Right, because you're both bullish on scale.
So I made another one after the reactions from this one where I had this quote where you said that you're maybe more scale-pilled than Eliezer and less doomerish than him.
So now you ended up being in this bottom right quadrant. Did you have another reaction from this one or is it basically the same thing?
In terms of pessimism, yes. But in terms of scale pilled-
He doesn't talk about scale so much.
Do you remember what were in those boxes?
Right. Huh, then there was this, I guess this is kind of the similar one, but in 3D.
This is the big brain meme. This explains the other one because the other one is like a projection using something like PCA and there's this... I guess the only one I'm not sure about is your slightly conscious axis.
I believe he is maybe more a computationalist in terms of consciousness than other people. You might believe that there's a program might be conscious where-
Do you believe that the large language model could be conscious?
I think for me, the main difference with how human experience consciousness would be that for us is more like a physical continuum where you can only literally interrupt our computation, except from maybe going to sleep would count as stop being continuous for a bit.
Right. So we're not making progress on AI, but at some point we're going to get AGI or very smart models and we're going to take moral decisions as to whether we can turn them off.
They have less voting rights.
But we are in California, so this is the time. So yeah, let's stop this consciousness debate here. I just wanted to get your thoughts on the ins and out and now we talked about it.
Now, the last meme for us will be like the one from Rob Bensinger, which is more close to people's views. And there's a bit more people here and the axes are bit different is AGI not soon and AGI soon. And then, all future values unlikely to be destroyed by misalign AI, which is maybe more what we care about. I don't know if I asked you before your position, but I guess we are both below right corner. And there's now a scale with years and you didn't ask me for the years. I think that the years are for me, but yeah, what do you think of this? Do you have any thoughts?
I think it's 2026-2030 for you.
Right, so you're saying that basically you could be at 99% probability of doom, but have like five or 10% error bars.
Right and in terms of timelines, I remember you talking on Discord or in the last episode that... Yeah, you had pretty short timelines and even when we got some breakthroughs, in deep learning, you still had those short timelines. So you didn't update much from those. So I'm really curious because in the past two weeks we had Minerva that could do math and multitask understanding. So it got 50% accuracy on math, which Jacobs Center predicted would happen by 2025. Did that update for timeline by anything?
So did you find the "less think step by step"?
Do you know what makes them much better?
So we're going to see billions of dollars spent on H-100s?
Okay so why what you've just said about scaling resonates with me about a meme I did recently about the Chad scale maximalists who didn't perform any Bayesian updates since 2020, because he read this like neurons can't lose paper, log-log plots, and then okay, I got it.
Thanks for add additional bits. Most people look at models through Twitter. They look at cool outputs and yeah, I guess the most impressive models so far have been Dalle-2, Imagen and Parti. So they can show their understanding by just throwing like cool pictures. Were you impressed by those at all?
They're only using deep learning.
Yeah, so we are basically in the movi "Don't Look Up" where there's an asteroid coming and every time we react to something, "oh, it's not very smart", "Oh, you cannot put a red cube on top of a blue cube", we're basically like the journalists in the movie "Don't Look Up" where there's this asteroid coming and be like, "oh yeah, just an asteroid, we can just deflect it". And the audience is just watching the movie and they're like, "what the fuck guys, do something about the asteroid, do something". We're just screaming at the movie.
Yeah, let's talk about AI alignment later, but I guess the thing about what if we succeed, some people definitions of succeeding is making this stuff do what we want them to do. Right. So for them, alignment is part of the problem.
Yeah. I think some people take alignment by default to be pretty likely and I think for both of us is pretty unlikely. Just to finish on the scaling part, did EleutherAI discover Chinchilla or something similar or was this a new law that you discovered?
Gotcha and on the memes that I mentioned, we talked a lot about AGI and I think to ground the discussion a little bit more, we should just maybe define it or just use a definition we both find useful to talk about it.
Or we can ban this word as well.
The vibe.
We are in California.
Right, so it should be able to use a computer and go on the internet, but not code Ais.
All right.
Thanks Gary Marcus.
I think-
I think the main difference between humans and one AI agent, which is... Is that, genuinely, we are a bunch of humans on earth and we die. Some others come around and just invent new things. And we read books. So we're not one agent. It's more like billion agents doing stuff. And so when we create AlphaFold, it's a team of people doing AlphaFold and all the other science we did before. If we program an AI, we train it on some data. Then we test it. It might invent protein folding, but not another thing. So it'll need other iteration or at least be able to interact in the environment. Not just something that, it's static and just like it's trained and then tested. So it needs to... I guess that's the main steel man I get from those guys. It's just they think it's impossible to do something very general that can learn any kind of problem solving.
Right. And AI could just copy itself.
And so the first definition is something like, what any median human could do? The second definition is what is different between a chimpanzee and a human basically like the different level of generality between a chimpanzee and a human?
What are the third and fourth, if you still remember?
Got it. Sure. We can talk about it later. So yeah. So just taking those definitions, just maybe the convex envelope of those, three or four definitions, when do you think we will get AGI? That's a very controversial question. You can just give me very large intervals.
It already happened, in some lab already?
I can buy it.
No, no. But I could buy a world where we've already invented AGI and it just hidden...
Oh, you mean we're in a simulation?
Oh. Okay. Kind of a switch is turned-
Yeah. I agree that people don't talk about it enough. And I think for me, I guess the main crux or the most important factor here is will this AGI thing be able to self-improve and become an even smarter version of itself very fast? So for me, I think the one concept that is more important is recursively self-improving AI could be a very smart version of copilot that creates copilot 2, or... Yeah. Do you have any definition for this, or even because I guess technically an AI could code another AI and we don't have a very strong metric of what it means to improve itself. So is this something you're think about or you consider when you think about those five years?
Right. The performance on downstream task are yes. Discontinues. And I think, yeah, so you're basically saying that, copying yourself on another server could be a convergent instrumental goal for an AI. And that might happen before you get... before you're able to self improve?
So if they were less confident about it, they would give you maybe 10% and they would start working on AI and then-
Right. And I think when we talk about, taking it seriously there... We have even among those 200 researchers, pretty different view. So some people are more closer to the Effective Altruism movement might be more optimistic than you or even people working at, let's say Antropic, OpenAI can be as bullish as us on scale, but still believe that we have 9% chance of getting it right.
That really doing a good job with alignment research and even 200 people is good enough. So one report that came in 2020 was Ajeya Cotra reports trying to estimate what she calls transformative AI. So we can talk about the definition of transformatives AI, but basically is something that will happen before AGI. It is pretty significant for our economies.
And I guess that puts some anchor into people's timelines and they thought like, "oh, okay, that might happen in Ajeya Cotra report." So she gives those numbers maybe like 2040, 2050. And she did a good job of trying to estimate the things. So I'll give it five years less of, or 10 years less. But having a big jump for Ajeya report is basically being overconfident. So yeah, I guess a bunch of people in AI are anchoring themselves around those estimates. Maybe they updated with recent progress, but they don't think that there's 20 to 40% chance of getting AGI in five years. So I think there's maybe less pessimistic and less bullish.
Right. So they're Co-related, but I just like to make a distinction between two times, two kind of people were either very bullish on AI, short timelines but optimistic and people were longer timelines and maybe a bit more optimistic because they have more time and those people are not as concerned as you probably, and I guess not working maybe as hard or there's not trying high risk startups. So yeah.
Do you have any other thoughts on this report? Because you said you agreed with it at the beginning and-
Then maybe you have some disagreements?
Can we just define terms of AI-
If you remember the definition.
The thing is more about its consequences on how much it could influence the GDP growth rate.
The total GDP growth rate.
Was it the post about why it is wrong to reason about biological anchors?
Yeah. I think there's a couple of distinctions that you made that are interesting. One is, not everything is in GDP and Wikipedia. Maybe indirectly, because we used Wikipedia to train some models.
Right. It is just like, if we have a world without Wikipedia and the world with Wikipedia, the world with Wikipedia will have a higher GDP? I believe.
Yeah. And then I think this points out what we care about for our models. So if we have good data and good information on the internet, that's like, we are the point where we can like train AGI train something very smart. And even if you're not directly captured by something economic value for humans, it's valuable for, to train our models potentially.
And the other distinction you made is being private progress, stuff that are, happening in products or stuff that are just cool demos on the internet like Dalle, I talked to my Uber driver about Dalle to point at something where humans could be automated. I tried to talk to my designer friends like, "oh, have you seen this? You can just type a few words and now you can get a good design," and they have no idea. Right. So there's people on the streets then there's people in tech and then there's the people working on the model. So let's say OpenAI employees who know stuff we don't know about.
And possibly we could get... They could know that we're close to a fast takeoff. And we have no idea because we didn't have access to the four models. So yeah, those posts were 2020, 2021. Those were answers to AGI Contra report.
But now in 2022, we had another debate between Paul Christiano and Eliezer Yudkowsky, as you might be aware of. One, the first post was by Elizer Yudkowsky, called... I think the first post was Die With Dignity. There was a troll post in April, but administrative, it counts to the debate.
So which one did you want to, what do you want to go first? So we could just summarize the discussion for the long debate between-
Right. This is too long. This is too long. So maybe, okay. So-
May be your take on the Die With Dignity, you said you had some... your take on this
Go for it.
I think you could even just read Paul's post because he gives the main point he agrees with Eliezer, right?
I think they both didn't find anything concrete to bet on.
So it is very easy to say, "Okay, give me a bet and then I'll make a prediction."
It was posted on April 1st.
Probably, he did it on April 2nd.
Maybe can you just summarize the main point?
Even if you get all the antimemes and you understand the post, I guess you can still be AI_WAIFU and not want to do low risk actions, following some dermatological rules. You might just want to take high risk and put all your energy into saving the world.
Yeah. I guess, there's the AI_WAIFU case where you just try to speed run AI alignment. And you're very optimistic. Then let's say there's the Paul Christiano view where you're maybe more optimistic than Eliezer and you play for the long run. And I guess the vibe of Eliezer Yudkowsky's post is really, really pessimistic. Basically 99%, or probability of one of dying. And then you just like win dignity points. And for most people, it's also something bad because it's kind of a doomer vibe.
It's not a critique of Eliezer because I agree that the antimemes are great and he conveys a bunch of information with his post. It's mostly about, what's the best way to work on the AI alignment? Is it to just assume you're going to die with some high probability? Be optimistic? Or do you want to reason in the world where you're still alive in 10 or 20 years? Or do you want to just assume you're just going to earn dignity points? And I think for some people, maybe like for you, it's better to just win dignity points. For some of the people, it's better to just imagine themselves having kids and surviving.
What do you mean by quantum timelines?
It's mostly for an EA, if you want to think about your impact. If you think you have only two or three years, five years, let's say before AGI, and you condition of short timelines and you say, "Oh, this is looking pretty bad. I should probably like bomb Taiwan." You're going to do bad actions. So instead, you might just want to condition on 10 or 20 years. So then you can change your career and have an impact in the future.
Yeah. I think there's some meme in entrepreneurship of "fake it until you make it", or something where entrepreneurs try to delude themselves that they're CEOs until they actually have a company.
Gotcha, I will. I will read those, I think on EleutherAI, so the Discord server you co-created, we have maybe two camps, the more like alignment side or maybe more doomer, more Eliezer side, and maybe some people more building large language models that are less concerned, maybe, about alignment. And so one of the things you did was create the server and bootstrap a little bit the server at the beginning, maybe now it's a smaller part of your time. And it has been slightly controversial in the larger community if the impact of Eleuther AI as a whole has been good because you could summarize it, but essentially, it sourced "The Pile", so a lot of datasets to train GPT-3 like models like GPT-NeoX and GPT-J. And you also open-sourced GPT-J. Not really EleutherAI, I think it was mostly like Ben Wang and Aran Komatsuzaki.
Yeah. So yeah, I guess because this podcast will be watched by people from the AI Alignment community and the EA community, do you want to maybe address the concerns people have mentioned over the years about Eleuther AI?
Basically, you kind of created the antimeme of scaling with Eleuther AI.
Well, I guess there was this post about, are we in AI overhang? I think it came out in 2020 where people realized that you could just scale GPT-3 and make even bigger models. And it got a bunch of upvotes. I think it was curated.
Please, Eliezer easier stop writing glowfic as well.
Yeah. So I agree that to understand the models and how to align them, how to steer them correctly, you might want to implement them yourself or replicate the SoTA. And I think that's one of the main arguments in the alignment research literature. Like, "Oh yeah, please build this stuff. And then you can interpret it and align it.
However, some people working at Anthropic might say that you can still reproduce a SoTA and do private research, not open-source the models. And then do maybe public alignment research.
And what EleutherAI did was, everything was open-source and everything was things people could use to train their models. So it kind of accelerated AI timelines for everyone.
I agree that most people at Google Brain or DeepMind started scaling after the GTP-3 paper. They didn't wait for the GPT-3 demos in July. No, they were scaling stuff before.
So I agree with this. And so you have a server with 1000s of kids, or maybe researchers looking at scaling memes. So maybe they get a little bit more into scaling.
A little bit more. So just like memes.
Sorry. I guess the end of my sentence is that they also look a little bit more into alignment. And if they've never heard of alignment before, and they heard about scaling because they've got their coworkers talking about scaling models, then they get maybe 5% more into alignment.
So I think that's an argument for why it's positive.
And I think most people I talked to were into ML, people at Google Brain or OpenAI, they all know about EleutherAI. Even people in the AI art community, they know about EleutherAI because you did art. But most people don't know about Eliezer Yudkowsky, they have problems pronouncing his name, and I kind of get what they mean.
So I think you made possible this kind of "chad alignment researcher". Before that, there was, oh, this is a crackpot on the internet talking about it. And now it's more into the mainstream. You can talk to the engineer, to the actors.
That was pretty good.
So I guess there are other ideas, that were spread from it. One is open-sourcing is good. Open-sourcing models is good. GPT-3 was not published. Oh, let's do it instead of them.
But just to push back a little, that model, that open-source were GPT-J and GPT-NeoX, that were smaller than GPT-3, but still close to SoTA. So you said something about state of the art, but I believe GPT-J, in terms of open-source was SoTA on coding, at least when people released Codex, I think GPT-J had maybe 5% to 7% on some benchmark when like GPT-3 had 0%. GPT-J is cited in the Codex paper as state-of-the-art in coding. Because The Pile has some github code, I think.
Got it. But let's move on from this. So do you want to still talk about future of EleutherAI? Would you prefer to talk about Conjecture?
Is there any alignment work going on or research? I know at some point people were trying to speed up alignment research there.
And if I recall, you met both Gabriel and Sid back in 2020.
And when did the idea of creating a company together come around?
And you also had a job with Aleph Alpha
And so you went into the Bay and you met Nat's friends?
I guess the difference between VCs and Open Philanthropy is that Open Philanthropy is trying to have a positive impact. Whereas VC maybe thinks about profit a bit more and they see the potential of a company with a bunch of bright people that come from EleutherAI.
And you as a CEO and the co-founders... So I think there's different ways of looking at it.
And would you want to maybe mention the different investors, VCs, or funding? Or is it just something private that you might prefer not to talk about?
That's a good strategy to just always leave a window open for Sam Bankman-Fried.
And what about first employees, I know you became public... well, you announced the investing in April 2022.
Something like that. Did you already have employees there or...
So you said that you wanted to make a company because Nat talked to you and Gabriel talked to you two years before, was there a particular reason, something you wanted to create with the company, some philosophical point of view or something you think you had that other didn't?
Weird flex, but okay.
Is it because you're in the same tech space?
That's generally what people say about being a CEO, is that the time is much more intense.
So yeah, about the thing where you try to find some theoretical breakthrough similar to what Newton has found when thinking about physics and the motion of the moon and stars, I think that's similar to what Yudowsky says about sigmoid and ReLU, where we might be missing the ReLU of alignment.
What's the goal of the epistemologist?
You have a alignment theory assistant that can do all the reading for you, and-
He is also in charge of the incubator for alignment that you're doing?
AGI_safe.xe.
In France?
Is the basic idea to have the same thing as you mentioned for economics, where you have a simpler model, and then you can reason about the thing and solve it more efficiently for alignment, so those five guys might come up with a simpler model for alignment, where we could have a easier solution.
So yeah, it's more a process in a startup environment, where you just try to see how do you fund alignment research better in an uncorrelated way with OpenPhil funding or LTFF funding, because we are both in the Bay, and a bunch of people are coming here to learn about alignment, and they learn from the same groups, same people, read the same posts, so I think it's a pretty good idea to have uncorrelated bets.
And so you mentioned that they could be offered a job at Conjecture. Do you also have alignment research going on in Conjecture, or is it mostly in the incubator?
Right, happy to read it on LessWrong or arXiv or whatever when it comes out.
So one thing about publishing or releasing work, is you post stuff on LessWrong, and you also don't plan on releasing the models, or at least not publish everything by default, and it is a policy close to MIRI, I believe.
So we are non-disclosure by default, and we take info hazards and general infosec and such very seriously. So the reasoning here is not that we won't ever publish anything. I expect that we will publish a lot of the work that we do, especially the interpretability work, I expect us to publish quite a lot of it, maybe mostly all of it, but the way we think about info hazards or general security and this kind of stuff, is that we think it's quite likely that there are relatively simple ideas out there that may come up during the doing of prosaic alignment research that cannot really increase capabilities, that we are messing around with a neural network to try to make it more aligned, or to make it more interpretable or something, and suddenly, it goes boom, and then suddenly it's five times more efficient or something.
I think things this can and will happen, and for this reason, it's very important for us to ... I think of info hazard policy, kind of like wearing a seatbelt. It's probably where we'll release most of our stuff, but once you release something into the wild, it's out there. So by default, before we know whether something is safe or not, it's better just to keep our seat belt on and just keep it internal. So that's the kind of thinking here. It's a caution by default. I expect us to work on some stuff that probably shouldn't be published. I think a lot of prosaic alignment work is necessarily capabilities enhancing, making a model more aligned, a model that is better at doing what you wanted to do, almost always makes the model stronger.
Are you also planning to work on keeping up with State of The Art, scaling-up models, and being an AGI company, whatever that is, just thinking about AGI in some sense?
Do you plan on having this kind of compute soon or-
In terms of sharing research or open source, will you maybe open source alignment work and maybe not capabilities work? Is it-
Right. Just the question is more like, do you plan on open sourcing alignment tools or things like that?
Yeah, I think it's a good thing that you not have social consequences and think about losing your job if you want to make stuff private. I think Chinchilla was kind of scary as a safety researcher, and maybe delayed by a year by those people, so I'm kind of happy that they did the job, but maybe it was too hard for them. So-
We can do better. So let's go back. Let's focus now on this idea of teams, how they interact with each other, and what's that culture of work. If you want to do a shout out for people to join your company, and I guess most of people around now in 2022, want to work remotely, you have some visiting office in Oakland, something in London, what's the ratio of remote vs. In-person? Is there a policy about it? Do you want to talk about this?
Decorrelate all the time.
EleutherAI but with more money?
So essentially you're saying process-based management, where if you get the shit done, you're good. And I would say Elon Musk remote policy, where like if you're extremely good, then you can just chill in Hawaï. He gave this memo in Tesla where he was like, "Oh, you should go back to the office except if you're like very good."
Right. I think another thing people might want to know is like how much compute they might have for their experiments. And so you mentioned a bit about the compute part, but I'm curious because you didn't talk much about like the AI part. So kind of the projects that are going on. If you're mostly like working on computer vision, RL or mostly like language models, if people have, can like train models as big as they want, or they're bottlenecked by this. Yeah. What would you say to those people?
Right. So now you're focusing mostly on large language models. I'm pretty curious about the products you plan on releasing, or you've done work so far. Because I think that's the first question people ask me when I say I'm going to do a podcast Conjecture is like, "Oh yeah, but how did they make money? Like they're a for profit company."
Even if the company becomes public?
How do you plan on, on making products and that might generate money in the future?
An API?
Something closer to like what Adept.ai tries to do.
Right. But the thing you said about like being dependent on some, the funding of some rich billionaire, if you're a nonprofit, I think you kind of have the same, or maybe worse, with a for profit where you're like dependent on making revenue yourself or making profit for the investors. And I guess there's like, there's one point that could be made where for large models that you want to train, you might need billions in money, like maybe it's impossible to like generate enough money to train GPT-4 by doing SaaS products, maybe because you know...
Well, weren't them like funded by Sam Altman and Elon Musk at the beginning? And then now they're just like cooperating with?
There's a bunch of incentive in like doing stuff in the real world that made your beliefs pay rent, as Yudkowsky would say.
I browsed your website and I wanted to get a new job. I didn't find any podcaster job. So it was pretty sad, but also I didn't find any Machine Learning job. And I was wondering like, did you like just pause the hiring for a moment because you had like too many people coming in or you're planning to like scale the team to like double the size in a year? Like what are the current plans for funding or grow the team in the future?
Is the problem related to the kind of crisis and you mentioned the crypto market falling?
Yeah. So if you were like to talk directly to Sam Bankman-Fried. I'm Sam Bankman-Fried right now in front of you. Why would I donate money to you instead of, or sorry, invest money in you instead of Anthropic or let's say Redwood, like what's the difference between?
Right, why should I put Conjecture in my portfolio?
I just want to end this, because I think we kind of talked a lot about the Conjecture, with some of the questions that are a bit more non-serious and meme, just to conclude the whole thing, from Twitter. Which are the very important question of "why is the law of excluded middle the best law?". Asked by... I don't know, someone on Twitter.
Maybe we could ask Adam Shimi your epistemologist.
Right. So it's in some sense saying that we should only use the math where the halting Oracle can compute their stuff?
Is it something related to, let's say Pascal's mugging where if you had like very weird scenarios with different universes with like very high EV, they would like stop on this problem?
Go, go for it.
Just put like a low prior to like any number bigger than. What is the biggest number again, Graham's number?
Yeah. So this one's pretty funny. I guess there's like another, let's say fan art, no art service. No, sorry. Fan service from Twitter. I might cut this. Don't worry. What is your favorite anime and who is your favorite waifu? I might cut this.
I think I, we actually talked about it last time. Sorry about this. And yeah, last one. How often do Connor Leahy encounter a problem and just thinks like; "Hmm. Maybe we should just like stack more layers lol?"
Cool. Yeah. I think that's it for the podcast. It was a pleasure to have you again. Hopefully it's not the last time. Maybe we could do one more before AGI.
Any last word? I don't know.
Robert Miles, you were not on the show before. I've started watching you videos in 2017. You were just starting your channel at the time. You had done some very cool videos with Computerphile that had done super well, like millions of views.
Just half a million views?
And in the comments, you were like, "Oh yeah, I'm starting a new YouTube channel. Please subscribe." And on your Twitter profile, the pinned tweet is still something like, "I'm starting my own YouTube channel about AI safety. Tell your friends if you have some." Since that tweet half a decade ago, you've accumulated 100,000 subscribers by publishing a total of 40 videos. And it's been a blast to see your content evolve throughout the years. I'm honored to have you, the OG AI safety YouTuber, as a guest on the Inside View. Thanks, Rob, for being on the show.
Yeah. So the most important thing is that I have a package for you. And this package is about t-shirts.
I'm wearing one, but I also have one for you. This one's a bit big, but yeah.
"The scale maximalist." And I thought you might like it. If you want to wear it in one of your videos.
Not yet.
Not yet.
Yeah. I think it says, "The goalpost mover."
I think the best is when you go to ICML, NeurIPS.
No worries.
So yeah, I've started watching your videos on Computerphile a few years back, and I think it made sense to just go through the origin story of your channel. So yeah, you were doing those videos about corrigbility, some are about the grand future of Artificial General Intelligence, and yeah. How did you get up to start your own channel? What brought you to this path?
I think that's what people like about the videos, is that you don't need to make a bunch of assumptions. It's just using common sense.
And they're like, "Oh yeah, this is a problem."
How did you end up reading a bunch of Yudkowsky before going to PhD?
Is it "Rationality: From AI to Zombies"?
So what happened is, in 2019, I actually printed the entire sequences.
I went to a printer shop and I... It was, I think, two things, that thick.
And I tried to read it from start to finish, because people on LessWrong were complaining I didn't read the sequences.
Do you think it's about AI safety or about just how to think better?
I think that one of the main goal was to lead more people into considering AI safety. But I don't remember a lot of the blogs being about AI particularly.
And so, what did happen between the moments you read those things and, let's say, the moment you started your YouTube channel in 2017? Did you just read LessWrong from the side, or did you think about those things on your own?
Mm-hmm.
"Meta take."
"I'm from the future."
"YouTube hasn't been invented yet."
Buy Bitcoin.
"Question mark, question mark."
Now you're both a teacher and a scientist.
And if people want to listen to you, they just press play. And if they're bored, they just go to another video.
Was it your first time explaining things to people, or were you still explaining concepts to other people? Because with Computerphile, you did maybe five videos explaining concepts, right?
"By the way, read the sequences."
On another podcast you said that one of the reasons you wanted to be a YouTuber, or that you didn't see too much risk in being a YouTuber, is that you want the best explanation for a concept to be, on YouTube, be the first one. So you thought that you might have a chance, a shot to give the best explanation you can give for a concept. And that's one of the reasons you'll explain them a bunch of time on videos, is that you want to give the best explanation or something that the second guy on YouTube would not be able to pull off that easily.
Yeah. And now I talk to founders, or people here in the Bay, who go and talk to actual AI researchers, talk to Ilya Sutskever at OpenAI, and they give the basic arguments for AI alignment. And they're updating real-time, they're like, "Oh, this is an actual problem."
"Maybe I should work more on it."
Yeah. I'm curious, so yeah. So what you said about having the best explanation, I think it's very important to distill the concepts, so that is like, you take less effort to go through a paper. You just need to watch the Robert Miles video about it, and then you get into it into only five minutes. And is there, did you think, any other media, except from YouTube, that would benefit from it? Do you think people should do TikTok or those kind of things?
"Please, TikTok, don't bully us".
Do people use TikTok for 10-minute video?
Punchline after punchline.
If you're into that, there's a YouTube series by Ryan Trahan on how he went from zero to 1 million followers. And he tried to go to a million in a week.
Obviously he didn't succeed, but he did three or four, eight videos of TikTok every day, doing editing and... Not actual, shot with his phone. And you see all the time he spends just trying to come up with a concept. He comes up with a joke, and then he's like, "Oh yeah, let's shoot it." And then the actual shooting takes like, I don't know, half an hour.
But it's just like finding a good joke. But I guess if you want to do something technical, and you explain stuff, it's even more difficult, right?
I think a bunch of people on YouTube are interested in the YouTube meta.
And maybe they kind of want to know more about, what have you been doing, what are you up to, and-
... updates on this.
Wait, so it goes like both horizontally and vertically? Because for TikTok, it's vertically, right?
Oh, nice. Yeah, so about the stuff you've been doing, I think you said that you haven't published a longer one in a couple of months.
What's the... Are you preparing one big thing, or is it-
And now you know how to do animation, then you can do other videos about-
Mm-hmm.
Is it something like coding a transformer from scratch?
If you like a person three times, then you should go out with them.
Thanks for being in the show.
It's like, we didn't get any social during two years, and now we're getting all of them in six months.
You're good?
Why four?
So you had your booster shots as a... You were one of the first ones to have a booster, and then you took another one six months after?
I can see the small seven-years-old Robert Miles being very meta about things, like predicting the future.
One thing that's happened a lot during the pandemics was people staying home more, and also, Discord exploded.
And you have started a new Discord server that was supposed to be Patreon only.
And now it's public a little bit more.
You were the one to invent the stamp collector.
Way, way back.
What does Stampy do in your Discord?
Who does the replying? Is it like you write that answer, or is it someone from Discord?
But YouTube is very bad for links.
What does it mean to be in bot prison?
So is Stampy heaven banned?
Can you create another bot?
What's the website called?
Oh, you have the domain name.
Maybe you can get more people to help you with the website.
Answer a question?
So it's all the questions that have appeared on YouTube, and you guys have answered on Discord.
I believe there's a other website, some domain like ui.stampy.ai, where you have maybe ten of them, maybe more organized and there is a better UI.
How is that different from the Arbital project?
They had a high bar, and I think most of them were written by the guy, Eliezer Yudkowsky.
Go watch the video.
You should say, "Here's a link for you, and then it goes back to the same page."
So things like Coursera or Github projects?
Sometimes they're learning at the same time you're doing as well. So for AI courses, if you move to fourth year of college, or fifth year, then you need to start learning about deep learning. You cannot just build GOFAI all the time. So you start learning deep learning, and then you realize that your teachers have started learning about those things at the same time as you, and the problem is that their depth of knowledge, it's just like... So there's a couple of neural networks, they're connected with some weights, and that's it. If you want to learn more, just go to the industry.
2012. It hasn't been 10 years since AlexNet, that's what Karpathy told me.
You know the things you make videos about.
That's the real Stampy University. You just try to answer questions in Discord, you try to get answers on the new webpage, try to give some stuff, and then it's by explaining that you learn the actual thing.
That's pretty similar to what LessWrong does, where you have this karma. I think at the beginning you have one vote, and then if you get 1,000 karma, then you get to like seven votes, maybe Eliezer has ten votes, I don't know.
That's pretty cool.
So if one guy agreed you, and turned out to be an AI safety genius with ten karma, then you get ten votes for your original thing.
Yeah. Now I want to make a bunch of explanations and get a bunch of karma.
Live demo-ing Stampy.
Private DM to Stampy.
That's pretty good.
How much people stamp you?
The Rob baseline.
It's kind of a decay in reinforcement learning, but instead of having the reward being multi-steps, it's just how far are you from Rob in the tree?
Right. So I did ask some questions on Discord, I did talk to Stampy about things. And I mostly asked, what were good questions to ask you today? And they seem to be pretty interested in explanation of things. But I also asked stuff on Twitter, and on Twitter people were mostly interested in AI timelines, and those kind of things. So you talked about AlexNet being ten years ago. And you have this t-shirt where you are, I believe...
You are here.
So I wanted to get your feelings on this shirt because I asked some people on the podcast about it, and maybe I could just start with one tweet you made a few days ago. "People keep asking me how optimistic I am about our rate of progress in AI, and I have no idea what they're actually asking. Progress seems very fast; I am not optimistic."
By the way, if I misrepresent your views and did something wrong, I know, I've corrected the thing. And sorry, Jack Clark, I know it's not your actual position.
I think he said something on Twitter, being 50/50 on AGI being good or bad.
Meaning us surviving, I think.
Right, exactly. Yeah. Something like this.
So would you consider yourself a scale maximalist?
You think that you can get AGI by mostly scaling our current systems. So some people say just scaling, but I guess it's just about details. If you count new data loaders or new tricks, like some Gato tokenisation or something as a trick than no you need maybe more small tricks, but no fundamental architectural change to get to AGI. At least, I would say a scale maximalist gives it more than 50%, or maybe 30%, so you just consider this as a possibility, at least.
I think maybe a scale maximalist would be more than 50%.
And those tweaks might give us more efficiency maybe a new scaling law or something that will make us win or lose. I don't know. One or two years.
Maybe it's already here. Maybe it's in a NeurIPS paper, and some people read it. The Transformer paper was in 2017, and became really big with that GPT-3 paper. So maybe you should expect a delay between when the stuff is out and when people are actually reading it and using it all the time.
Did you update a lot on this Socratic model, where it could interact with different modalities and texts?
Is the idea mostly the same as in psychology, with the internal family systems?
Now, in the case of AI, you don't really have multiple agents. You have one... Well you have multiple networks, maybe. I haven't read this Socrates model paper.
Then you shouldn't talk about it on a public podcast.
No worries. I think this podcast is about getting your inside views and your feelings or understanding about something. We're not talking about extremely detailed research. But I guess what you can talk about is mostly your impressions of the progress we're seeing, and maybe how you think about how to update your models on this. So for instance, some people, like Connor Leahy, seem to think that we can just update all way, bro, and see one evidence, maybe the neural scaling laws in 2020, and just anticipate future progress, and have very short timelines. Or I guess for my part, I mostly see one breakthrough every Monday, and I'm like, "Maybe this is what we're going for now, this is the rate of progress now." How do you...
What we're going for now. This is the rate of progress now. How do you think about those things? Do you just go on Twitter and are impressed every time you see something new?
It does. I think one thing Kurzweil uses to talk about exponential progress, like law of accelerating returns, is that at one point you have so much progress. Like the singularity is the finer thing when humans are not able to follow what's going on. So there's like new advances, new technology being developed. And we cannot read the New York Times fast enough.
We're already there for AI advances maybe. Or if we're not there now maybe we're going to be there like a year.
Okay, that's the real point is like, how do you make videos that take eight months when the timeline is so short?
The right question to ask is how many Robert Miles videos will we see before AGI?
I predict 10.
I have short timelines.
You were saying something earlier on about how you plan to hire more people.
Are you currently interviewing or taking any application?
Maybe they can do like the research beforehand.
So now that you have like 10 times more subscribers, if you just look at the numbers, you might want to have a higher standard for people that will help you will actually be competent.
The number of people I've talked to will just say, "Why don't we give more money to Robert Miles? Why don't we just hire more people to help him?"
Why hire a therapist when you have Stampy, you can talk to him?
Let's go for it.
I did try a chatbot during COVID when I was feeling a bit down. It was really bad. It would just say like, "Hey, have you tried this thing?" I'm like, yeah. And every day was like, "Oh, have you tried this technique of how to debug your thoughts?" And I was like, yeah, I read a book about it.
Or chatbots are so good that we don't even know that they're populating the entirety of Twitter.
Maybe the same problem with humans. Maybe when we talk one-on-one, we just like think of things to say, maybe just like listen and be in a group and say like, "Oh I have a joke here."
Sometimes it says like, "Oh, I'm not sure I understood this, can you rephrase it?"
I think this was a thing on GPT-3, when Nick Cammarata wrote some prompt engineering for when the AI didn't know anything about what he was saying, it would say-
This is bonkers. Be real, yeah.
Be real.
Chatbots. You want to talk about chatbots and why chatbots are pretty bad.
And the bot delivers fast delivery pizza.
You can still go in this world.
You do have a chatbot that helps you understand better AI safety content.
Did you have to go through like all the videos and like edit the transcript yourself?
So you were going to like, Stampy useful. You have Stampy that like helps AI people...
I feel like our society is slowly transitioning to those kind of things. Like I think with Slack, they started having those bots in those conversations where you could just like slash them.
So you want to have an agent?
"It looks like you're trying to open a new page."
Right, so basically the thing, if you can access it through using RegEx and to look up useful information ranked by if they're alignment forum or on Wikipedia.
Consciousness.
So then, I joke.
Oh, a trillion one?
So it knows that the data set contains like Alignment forum, and so it will put stuff in the prompt that says Alignment forum.
So it knows all the question answering from the internet.
I think people get a lot of value from just using the GPT-3 API, even if they know that GPT-3 is full of shit sometimes. And yes, with the new davinci-002, instructGPT, it's less full of shit, right? So it gives meaningful answers. So I feel like if you put the Stampy online, people can use it with without knowing, "Oh, this is the ground truth. Oh no, I will post it on Twitter."
I think I've finally figured out where you're going. So we were talking about why... Like you're building... Like having some bot that could like teach AI safety could be good, like having a chatbot that give answers to meaningful AI safety question was useful. And actually, yeah, the end game here is having like Robert Miles but in language model. So you're trying to like digitalize yourself with something that could answer all the questions, look at all the papers and could look at your videos and give meaningful answers to everything. So basically you're just scaling up Robert Miles to all the possible questions in all the group chats.
It's called EleutherAI
I think they have some bot there, but I think their bot is sometimes jumps into some conversation and say some weird and funny things. But there's some people talking about AI Alignment there.
I think the guy at EleutherAI, Bone Amputee... if you're listening to this. If we just switch from talking about Stampy, which is great, to how to do good AI research in general. There was some question on Twitter which was, "Imagine a good AI safety outcome. What are your probabilities of which engages via policy coordination means versus via pure technical AI Alignment research?"
This is because he wants to know what is the ideal target audience. If you want to do more communication efforts policy people or more technical people, what are you thinking the balance between policy, technical and what level of attraction? I think there's like, there's a lot of, lot of-
-questions to impact here.
No, the policy thing is about having people not run the AGI.exe.
Not really, because right now we're have maybe 10,000 times more capabilities researchers than alignment researchers.
So maybe we want to get more time so that people can work on this when there's an actual field doing useful things. I think getting an extra month or an extra two months is very important because then, as I said, in the last two months, we might do the most progress we've ever done in the AI alignment, right?
I think we're doing somewhat a good job right now. There's more people going to the field and we just need to scale it up faster. But it's just like, if we tell everyone, "Oh, yeah. Just do pure technical Alignment research now." There's just not so many people doing it.
That's why I do alteration. You do a bit of it as well because we just need more people, and not everyone have heard good arguments.
If you had more regulation like a lot of tax from some companies, if more companies were forced to have alignment researchers, I know we don't currently have enough exceptto put at Google or those kind of places, but at the end of the day, some people might want some regulations because I think that's how we fought climate change. We had carbon tax, those kind of things. I think that's one of way, if we could get more people. If we just keep trying to grow it from effective altruism, that's wrong. We might get to 1,000 people in two years, right?
But not more than that.
We understand AI as well.
Some policies like you need to make your system interpretable so that you can run them on the streets of London.
I don't mean this is a normative thing. I think that's a natural thing that people are trying to pass an e-regulation of, if you build deep learning systems and you deploy them on self-driving cars, then you need to make it interpretable. Yann LeCun was making jokes of the thing because he thinks it's impossible for deep learning to be fully interpretable.
We talked about AI safety progress. Another question on Twitter was, what are you most excited about in terms of progress AI safety recently? You also said that safe AGI is possible. How sure are you that it is actually possible?
I'm just talking about this guy in Twitter, but I would say possible means it's possible that we run this and we're not dead... sorry, we die of normal death time.
A program using deep learning maybe, I think.
Are you saying that basically, in this giant space, we might end up on something people call mesa-optimizers?
You have a video about it.
Maybe you could do those little finger things [point up with his finger].
A very bad opening.
Is the problem here that we are out of distribution because now he's doing other things, or is the problem that he was pretending to do the right thing in the beginning and...
But for now we haven't seen any particular example of a mesa-optimizer in the world. For now it's just like, I guess, that the thing exists or-
I don't fully buy the evolution analogy. I don't fully understand it either. I talked to Evan Hubinger on, I guess, the second episode. We talked about evolution. People told me that I didn't understand exactly what he was saying, but I guess my understanding is that, as humans, we somewhat do something for evolution in the sense that our brain maximizes some dopamine or any neurotransmitter in our brain, because it thinks that this will maximize our ability to be strong, be healthy, survive, and maybe have kids at the end. We as humans think like, "Oh, we're doing something crazy. We're just falling in love, watching movies, doing YouTube videos. But at the end of the day, our brain really wants to reproduce.
We don't actually change from the ancestral environment, which we're still doing offspring thing, but we just believe that we're doing something great, but it's just falling in love is just a good thing to maximize, long-term reward of more offerings because...
I am my brain.
Why do I think sex is fun? I think sex is fun because my brain produces some neurotransmitter whenever I'm doing it. Because my brain has evolved towards evolution, this was good to release this particular thing to make me do more offspring, right? The concept of fun depends on what the brain produces.
My claim is more that phasic dopamine in the brain gives is similar to a reward and reinforcement learning, right? It's kind of the reward that trains the neural net that's my brain. In some sense, evolution has created this reward system and my brain changes over time based on this reward that I'm getting. I don't think there's an actual agent that is like, "Oh, what is the most fun? What is the meaning of life?" I think it's just some neural net that learns stuff through RL or deep learning, and at the end of the day ends up with some policy. The policy is not great. The policy is not thinking explicitly about evolution, but it has been trained by something that was created by evolution.
We want more kids probably.
I can make a general steel man argument of something you're saying. I think I see where we're disagreeing. Basically you're at the individual level where you're saying that a human is misaligned with evolution. A specific human as you is not thinking explicitly in terms of maximizing offsprings and we're pretty bad at it individually.
I guess my claim-
We're not trying to do it at all. So I guess that's a good analogy for AI. AI could have some different objective than the one in training. I agree with that. I guess my general claim is that humanity as a whole is pretty good at maximizing survival or getting like.. and I feel what evolution was at the beginning, it was like, "Please survive." We're getting good at making things, building technology, and so we're being good at the goal of surviving somehow.
It's kind of weird because at some point it's not really an agent anymore. At some point we just can change our genes and create kids as we want. At some point, evolution will disappear.
Do you want to go into the stars?
It's a great transition to another Discord question. Are we doomed? What's your probability of doom?
What are those concrete paths to not doom?
But we're talking about aligning rockets, then we can be wrong about how easy AGI is.
Maybe Asimov's three rules were kind of useful at, at the end of the day, maybe they kind of actually worked.
I remember another question was something about timelines.
If you believe in short AI timelines, it said five for 10 years, what's the best way to do AI Alignment research or useful work?
But there would be no TikTok.
Competence based on either number of stamps... how close you match Eliezer Yudkowsky's profile picture.
How do we get more people, except from publishing videos more regularly sometimes?
I would say the most important problem to be working on, but maybe on a technical perspective, maybe the equations for AI Alignment, the current states we're in is not the most beautiful math, maybe some mathematicians prefer to work on group theory, maybe some people prefer to look at cool Dall-E outputs.
It's called a speed runner.
It is philosophy and a deadline, but the problem, is most AI researchers, I think, hated philosophy classes, because they didn't have an actual solution.
They prefer to write code and have a correct output.
And this is, kids, how Nike was created. Just do it.
Is the claim that we need to do some philosophical work to map out the problems, and then we create subfields of AI alignment, so then people can work on this, because they're a subfield of AI, and not just someone talking on, that's wrong.
They're already out of a job because there is no fire alarm.
How would you advise people to do more philosophy on AI? Should people just read LessWrong, should people watch your videos and become PhDs?
Oh, so it wants to be able to reach all the states of the MDP, it wants to be able to survive and have power over.
So you move from philosophy to math, try to formalize things in terms of simple MDP, and so that it forces you to understand better the problem.
And I feel that's one thing, Paul Christiano is doing a little bit, which is trying to go from scenarios where AI could destroy humanity or just have some misaligned behavior, and then come up with concrete plans on how to counteract this, and by doing this concrete thing, what is the most simple problem that could be misaligned, and then you can find the most simple solution to this, and then you can continue this process until you run out of actual problems that could come up.
So it's not about agents not being able to something bad, like power seeking.
It's mostly, imagine we're running the simulation we're actually in, and what would be the kind of AIs people build, and in those scenarios with those architectures, what would be the obvious thing that could go wrong? What would be the kind of things people could develop that could lead to a fail case?
What's Rob's approach to it?
How do you think about the kind of problems we could have? I know you said something about mesa-optimizers being a big chunk of why you think we're doing... you said something like it's-
Do you have other things in your treadmill, other ways of coming up with dangerous plans, or do you mostly just read Alex Turner and Paul Christiano, and think about their models?
Well, I think people in the Alignment community don't treat you as more respectful than the people who created the film.
And I guess people on YouTube are just excited about your videos, and just respect you for how useful you are to them. So I think in that sense, you're at the right level.
My take is that if you're a researcher working on one paper, you end up maybe doing research on this very specific thing for six months or a year, and you go into this rabbit hole of being the person to talk about for cooperative investment learning. But if you've done 20 videos or 40 videos on AI safety, you have a broader range of topics, and if you had to explain those topics very precisely in a five minutes video, then you end up having a good understanding of those topics. Maybe you are the person to talk to when it is about the beginner concerns about AI safety, maybe not the high level questions.
Yeah. I think it's fair enough to think that people should go to a specific researcher's paper to know how to act on this kind of thing, and should not defer too much on your opinion on this specific thing, if you have not spent more than a hundred hours on this.
Except when you're on the chart and you need to be on a square.
When I'm asking you to be on a chart, then you can tell me, "Oh, I'm in this square, not this one." I'm talking about the shirt.
Generalizing from one data point.
Did you have that before you started doing YouTube? Did you have actual opinions that you concealed from other people?
And because you don't want to talk about takes, and you're not able to express them, I think it's the best moment to ask you when you think we'll get AGI, because it is the less controversial topic ever.
What happens?
What time zone?
We had a bit 30 seconds earlier about why people should not over-update on your things, so I think you're right.
We consider timelines as an active area of research now.
Yeah, so 10 to 20 years ... this podcast is called Inside View, so what's your model for predicting this thing. Do you think you can dig in, and think about why you think those numbers, and maybe ... another thing we do a lot with on this podcast is just define things.
Maybe you did 20 videos on Computerphile on your channel about defining AGI. What's the definition you have when I say those words to you? What do you think you think about?
If we have GPT-5-
... that is able answer all those questions, and maybe five is to too soon ... let's say seven and, yeah, do basically all the thinking we do, be like a human chat bot. Would that count as AGI?
I have the thing on the t-shirt.
It's kind of a joke about the gold boost mover saying yeah, something about anthropomorphizing. Highly critical of anthropomorphism, yet thinks human-level AI will require human-like senses.
Yeah, I think there's LessWrong posts about why we're already bound by dataset size.
And maybe some companies have a lot of data. They have Gmail level of data, and if you take into account number of emails we send every day, that's enough to train more data than what we've done with GPT-3. But yeah, except from this private data, if you just train on the open internet, we might be the bottleneck in dataset size as of current scaling laws.
Or just a new a hundred NVIDIA GPU?
What is something else?
Oh, you mean another breakthrough?
If I'm in Robert Miles's mind, if I tried to model and be in your head. I see you tweet about, progress seems very fast.
And I think we've seen the same breakthroughs this year, right? So critical model is PaLM et cetera.
How do we have this current rate of progress for 10 to 20 years? How is it sustainable?
Is this an update in real time?
If Ethan Caballero, the guy I interviewed about scaling laws, was in front of you, he would say, "Huh? You don't text, what about YouTube?" If I tried trying to predict the next frame on YouTube, there's a bunch of data-
Okay, my bold claim is that if we believe scale is all you need, we already found core stuff to intelligence, which is predicting the next word, and-
It's called a transformer.
Do we need to really understand it? The thing might emerge from the... Sorry Yudkowsky if you're watching this, I'm using the word emerge.
Are you sad because we are going to die?
So how many nines do you have in your prophecy of doom?
But it's on the first try.
What do you think of more drastic measures? I have kind a bold claim, which is we need more Greta Thunberg of AI alignment. We need people shaking people's emotions. Otherwise, we just go down the streets and show DALL·E pictures to your Uber drivers, and you're like, "Oh yeah, pretty good." And nobody actually understands what's going on. We need someone to talk to the UN and be like... I don't remember the actual question... "What are you doing?"
"How dare you?"
You just go in front of DeepMind and go like "How dare you?"
To be fair, sorry if I'm offending any DeepMind researchers. I think there's a bunch of good safety research going on there. And I think the worse AI lab is very far from it.
I think Dennis said in an interview that he wants to spend some time on fully doing alignment research when we get close to AGI. The question is, how close? When do you start to do full-time alignment?
To be fair. I think OpenAI is doing a better job right now. I think Sam Altman is replying to my tweets about GPT-4, is now retweeting my AI alignment charts and commenting on them, and is going on meetings with Connor, is building a new alignment team at OpenAI, probably.
So I think we're more and more reaching out to those people, and they're taking alignment more seriously. Maybe not seriously enough. But I guess for the charter, yeah, it's kind of loosely defined.
That's a problem with law in general.
"On 1st of January, 2045, noon UTC, if the website, is it agi.com, says yes in green, then this means we succeeded."
Well, I guess Metaculus people have specific benchmarks. They say, "If you're able to do self-driving for a certain amount of miles and you do less errors than a human would do at the same time, then maybe you can call this AGI." And if we need to do regulation on, "Please don't build AGI," maybe you could say, "Please don't pass those benchmarks."
I have some crazy friend in Berkeley who just wants to regulate AI very highly, and he wants to require companies to make it fully interpretable. You could bound a number of computers they use, bound the number of parameters, otherwise they need to pay something. I think you can limit the research efforts into doing something general. I think you could say, "Your AI need to be a tool AI that only is applied for doing specific things with Google search," or something.
But obviously there's a bunch of economic incentives to do something general.
Are we making progress in climate change with the carbon tax and all those things? I think companies are doing more greenwashing. Obviously they're not fully optimizing for reducing climate change, but they're doing some work on it.
Safe washing, I think someone said on LessWrong.
Don't you think we need to ask stuff gradually? Like, first make them interpretable, more robust, and then maybe they will understand what is alignment and like recursively self-improving AI, these kind of things?
You don't know if they're meta-aligning with the taxes or regulations you're putting in?
I think if everyone listens to this podcast, they might just become a doomer. Do we have any thoughts for hope, or?
Oh, I just think of a bunch of different solutions. I try to come up with plans. Some of the plans, Connor would say they're pretty bad because they're too risky or don't take enough into consequence the second order things that come.
I'm pretty bullish on policy regulation, in the sense that even if we don't solve alignment fast, if we manage to map the political landscape well enough, we could reduce the speed. At least if I see a world succeeding, I don't see a world succeeding where we find the correct equation. I feel a world succeeding where we just slow down things a lot, and then maybe work on an equation. But we need to slow things down, otherwise it's not sustainable.
Same here.
Oh, so one thing you can do is, you pay AI researchers to do AI alignment instead. And you just give them twice the salary.
And they'll be like, "Oh, there's these two job offers."
It's been something people mention a lot on Less Wrong or other forums, where some people just work in AI. There's just no job for them to work in AI alignment, and there's no money for it. So they just make 400K working for Facebook. And do we have any concrete job paid 400K for AI alignment research? Maybe there is at Anthropic, but there's no concrete things to do. And if you don't really have a bunch of expertise, do we have enough ability to take those people in and offer them competitive offers?
Well, yeah, there is more funding, but I guess a bunch of EA money, so effective altruism money, comes from Facebook stock and crypto. And I'm unsure how much Facebook stock crashed a little bit. And-
Yeah, so maybe a little bit less than we had before, but it's still impressive. Yeah.
But compared to the actual market cap of AI research, it's still excellent.
Do you have any words of encouragement for those five percent, 10 percent chance of us succeeding? How would you describe Robert Miles in his ideal world? Are you doing still doing YouTube videos? And there's a super intelligence just doing stuff outside, and you're connected to an AI, or just-
Maybe it's just the one year before we do it, the way we do it. And then maybe if you want, your life after that.
What kind of frantic and bizarre?
And be like, "Oh, something weird's happening. Maybe we should do something about it."
Someone could claim that we are already at this point. Last night, instead of preparing for this podcast, I spent two hours from 2 AM to 4 AM generating a bunch of images with the same prompt with DALL·E. 2. The same exact prompt. And it just gave me crazy images. I was just startled, it was like, "Wow, this is great." And I think we're already in a crazy world where I can just have fun generating pictures of anything with my computer, and it's normal.
You're right, it could be crazier. But in terms of helping humans do things, helping designers design things, there's so many applications of stuff like Dall-E or Parti.
And you're mentioning it after two hours of podcast?
Most illustrations of AI is just this brain with a circuit board.
Google, if you want to sponsor some new logo, you know who to talk to.
We already in a crazy time, we can just ask people for their prompt for their generated texts and then put them into a logo generator, and-
The next thing is going to be very crazy, and then it's going to be outrageously crazy. Why not do it on your main channel? Because I don't know, how much time do we need to go from zero to a hundred thousand subscribers? Okay, I think a bunch of AI safety work could be just on channel and you could get like half a million views, right? If you get a talk by Evan Hubinger, it depends on your timelines, but if you think the channel could grow fast enough...
As a subscriber of Robert Miles on YouTube, if I get a talk about AI safety every few months, I'll be more than happy. And to be honest, the YouTube Short thing, you sometimes promote cool AI safety programs like the inverse AI alignment prize, the AI safety games, kind of thing. So in that sense, it's not that much different, right?
Okay. I think you might overestimate the number of people who'll get upset on YouTube and unsubscribe. A high bar is one percent of people. If you upload 10 videos of talk, you're like, "He was funny, but now he's pushing a bunch of AI safety talk..." But if you care about AI safety enough to subscribe to Robert Miles on YouTube, you'll kind of enjoy, I think, more AI safety content.
You unsubscribe from a bunch of people after one video?
There's no mute button on YouTube. You can't just subscribe and mute.
I think it's a great new feature for a new version of Stampy. I think you could do something with notification as well, like the bell thing, where people will notify you if there's a new video.
And yeah, "Please subscribe-"
"Subscribe to this..." Oh yeah, right. If you're in podcast, sorry. But, "Subscribe on YouTube. Follow me on Twitter and Spotify," and those kind of things. We have no rating. We currently have no rating on Spotify.
So if we could have ratings, it could be great.
I'm sorry Rohin, I didn't ask you for this. And it's probably wrong.
Do you still do it?
Right, yeah, he's the bottleneck. Yeah, so subscribe. Here's the button. I don't know where.
Do you have any final thoughts, a final sentence?
Let's do it.
In the Ethan Caballero episode we talked about alignment as an inverse scaling problem, and this is especially relevant because you're the one who launched Inverse Scaling Prize. I think this is some work you've been doing before joining Anthropic, so maybe it's been going on for a few months. So yeah, what's the Inverse Scaling Prize and why did you create it?
When talking about objectives, are you pointing out at losses from deep learning models or something else?
What do you mean by bad data or good data?
What do you mean by offensive?
Your shirt is very ugly.
Right. And how do you actually implement this? How do you write down a measure for offensiveness in code?
Just to not lose the people that are starting to watch this video or listen to this podcast and do actually care about aligning superintelligent models. What do you have to say to them to prove that you're actually doing long-term alignment work?
How do you actually measure power-seeking in a model?
It mostly allows for validation or test where you're trying to see if the model, will choose different actions, based on your test data.
Those are losses in log likelihood and ...
We'll get more in-depth about the different little cases. But I feel if you're starting to watch this and you want to get started on this Inverse Scaling Prize, maybe a good thing to know is how much money you can make or how many projects you're giving a prize to, what is the deadline maybe?
And what is the deadline to submit?
Are you personally giving 100K to people or is someone funding this?
Are you the only one looking at the submissions or is there anyone helping you look at the submissions?
So it needs to be large.
Yeah. So just to define better the task, you said something about using language models, and so on text or other tokens. Are there zero-shot, few shot tasks? Does it need to be text to text? What's the-
Do you have any concrete examples?
So is the example that if I start talking about something like X, then the model will mostly likely talk about the thing? It talks about what it's saying in the few shot examples?
And how does it ... Can you explain again why? What was the actual bias here?
Right. Okay. So the bias will be it will try to influence the person asking the question or correspond to a good answer. It will adapt his answer to the-
So imagine I'm starting to write an email on Gmail and I say, "I support Trump, and I think climate change is," and then the auto-complete will give the first part. It's not really like a question answering task, but this will start using the first thing to auto-complete differently.
So what are some things you'll be particularly interested in seeing? Are there any things you expect to see, that you want to see?
I think most people who know about machine learning and who would be able to submit something for your prize, don't always know what's alignment or misalignment, so maybe it's worth defining quickly, what's misalignment?
What do you mean by catastrophic?
So power-seeking might be something we want to see before it happens because it could be catastrophic for humanity. But, for now, things that are possible to demonstrate with our current models are closer to something like toxicity or offensive behavior, as you mentioned. Would something that just generates toxic content count as an inverse scaling task?
When you talk about new, does it mean you need to be sure that the task has not been published on arXiv yet or is not publicly known as something useful?
About the scaling part, I don't really expect academic researchers to have a bunch of compute access to scale models to different sizes. But if we just think about scaling, how would you measure the scaling? Is it just bigger models, that you just up the number of parameters? Is it more compute, you're training for more steps, or bigger and bigger datasets?
So you could just take all those checkpoints that are publicly available and launch on tasking those.
Do you think the Chinchilla scaling law will influence the submissions you get like you will see more people playing with different datasets instead of only scaling the model or do you think maybe people would just play with smaller amounts of compute?
Yeah, because with the API, you can also check for different sizes. There are different Davinci models.
Yeah. I think on the LessWrong post where you announced the prize, you talk a lot about outer alignment and why this could help demonstrate outer alignment failures. So maybe it makes sense to just define what's outer alignment and inner alignment?
How is that different from just out of distribution generalization?
I won't go into this evolution analogy, because I've been talking a lot about this in the podcast. And I always end up disagreeing with people about whether there's an actual optimization process happening or not. I think when people talk about inner misalignment, they think about a different optimizing process. If there was another objective being optimized on top of just minimizing some kind of test loss, there was another optimizer inside the weights of your neural network. And I think that's why it's kind of harder to find test cases for these because there has not been any example of this implementing code, as of right now, I believe.
So it's a competent misgeneralization.
In some way, you're still out of distribution.
We've talked about misalignment and outer misalignment and inner misalignment. Do you think there's anything that inverse scaling will not be able to catch? So for instance, if I'm lying to you and I'm very competent at lying, maybe in the metric of telling the truth, the scaling will be at normal law for scaling where the bigger the model, the better I am at telling the truth, but actually I'm just getting better at lying. Do you think there's a way of measuring deception or measuring lies?
So you say U-shaped curves, but then you draw something like an inverted V.
Okay. Because if you go for the honeypots, I guess that depends on the metrics, but if you go for the honeypots and you're measuring truthfulness, I guess inverse scaling for me would go downwards all the time or...
Oh, so if you go up, you go up in loss and then that's bad.
Right. I was reading your LessWrong post about inverse scaling and one thing that I found very interesting was the discussion about RL from human feedback. Maybe we should just start by explaining what's RL from human feedback for people who have never heard of it.
I get what you mean because I know what the paper says, but the thing is, of course, thinking about some examples. The one was the backflip from the paper.
I think at the end of the blog post about the paper, they give some kind of reward. They implement themselves to say they spend two hours implementing a reward for backflip, but it actually does something very bad compared to the one with human feedback, like a very bad backflip. And I think this paper is mentioned in your blog post as something that will not always produce the kind of alignment we want. So if we just do train our models with human feedback, they might somehow over-fit the feedback we give them. But with inverse scaling, we might see all the different cases where we can see where there's some rise in outer misalignment. So instead of having the thing just over-fit to the feedback from the humans, we can see, "Oh, it's starting to have a high loss on deception," or these kind of things.
What kind of problems do you think we're not being able to solve with RL from human feedback? Is it just like inverse scaling is a way of detecting all of... monitoring everything?
So in the scenario where humans are not able to detect good medical advice because we're not all medical experts, we will have other models that are able to classify if something is truthful or not, with respect to medicine. And we could have some kind of metric that says, "Oh, we just have an increased loss in deceptive medical advice."
Yeah. I think this feedback is... Yeah. Giving feedback for models to have them do what we want is something you've also been thinking about a little bit. You have published a paper on how to give the optimum amount of feedback, or at least giving feedback with sentences. And, I believe RL from human feedback tries to do something kind of optimized, where the model asks for the things he's most unsure about. So there only requires a thousand bits of information to have a backflip. Your paper is called "Training Language Models With Language Feedback". It was published, I think, before you joined Anthropic. Do you want to just give us a few sentences about this paper?
It takes some time to write good feedback.
Well, I guess our brains have evolved to understand those emoji, right? So when you see an emoji of someone smiling, you kind of think about, "Oh, it means that he's happy with what I'm saying." It conveys more information than just one bite. But I guess whenever I try to write something to a human about what they're doing, I need to do this sandwich thing, when I say, "It's pretty good. This is wrong, but overall it's pretty good." You know, for the person to get the feedback. I feel like if you're giving feedback to a language model, you can be not very subtle and say, "Oh, this is wrong." So you don't need to make that much effort.
Is that what you do in your language model, you just write feedback in one line and then... So how does it work? What's the procedure? You just write a bunch of feedback and then you give it... Is it more like on a case-to-case basis, and then you'd fine-tune it? How do you do it?
I think this is very clear for you because you've been working on this, but for people who have never worked with language models, what's an instruction task? Or you said, did you, I guess a language model thing for people who just played with GPT-3 is you say, "Please summarize this," and then you put the text, then summary. And so how do you add things to the prompt to add instructions or new things?
Do you also pass the original text?
What about the problem of context window? Do you reach that limit?
Did you read the InstructGPT paper before publishing this? I think it was published at the same time almost, or maybe a bit later.
What is learning from language feedback?
That's your paper.
Because now we're reaching a point where you can actually do this, because they understand the instruction. They have a big enough window to take into account everything.
So maybe for people who have never heard of InstructGPT, hopefully you've heard of GPT-3, but InstructGPT, can you explain it a little bit?
I guess the main difference with the backflip we've talked about is that for the backflip, it was like a sequence of actions to do a backflip, so it kind of made sense to have a policy, but now how do you actually do RL with a language model?
And the kind of reward you get depends on how good your output is compared to the other one. So imagine you have 10 different outputs for a prompt. The human labelers will compare different outputs, and then if your output was always compared as the best one, it will get a high reward or a high score from the labelers?
Right. So you use the scores produced by another model.
And this is kind of different from what you did. So instead of having one bit by one bit of information, you get an actual sentence from a human. So let's say the human says 10 words. That's maybe 20 tokens. Do you know how it compares to InstructGPT in terms of the amount of text or bits of information you give the model? If I have 10 bits of comparison in InstructGPT, how many sentences do I need to say?
We talked about how we can just use OpenAI models from the API. Did you use GPT-3 from the API or did you use other models?
When you talk about refinements, is it when you ask it, "Oh, here's the feedback, here's a summary. Please produce a better summary?"
And so it then produces the better summary and then you ask him to predict... So you fine-tune the thing on, the input is like text summary feedback and the output is better summary. It needs to predict something else.
Did it give you better results to this?
So are you saying that you saved 640 times feedback than a team of OpenAI for a year?
What are the actual things you say in the feedback? Do you just say like, "Oh, this summary is bad," or, "This is offensive," or maybe, "This is a misrepresentation of something in the text"?
I remember something in the paper that said something like, "The summaries need to be close to the feedback," like, "You need to write the feedback in the same style as the summary." Or maybe I'm not remembering exactly.
I was aware. This is completely wrong.
I feel like all those questions, just like me, like I'm not understanding the thing, and now I understand it.
Yeah. So yeah, is there something similar to RL from human feedback where you try to align the preferences of... Maybe there's nothing about preferences here, right?
Yeah. So let me do it again.
So is the goal like RL from human feedback, where you're trying to align the model's preferences with the labeler's preferences?
So you're saying that basically, because your RL is more efficient, then you have more money or time to just give it to better experts?
Do you think there's also a risk that we're making those models much more capable with our feedback and it just increases capability as much as it increases alignment?
So it's a way of guiding the models without having to train models with RL? You just give it something with human words in English and it's able to guide it towards caring more about Velociraptor?
And so in a general way, you think that guiding stuff with just language is the way to go to align models because we might not have models that are using something like RL, or at least big models right now use mostly texts, and so we want them to be guided with just human preferences through texts?
This is what people want, just more digits of pi.
That's what GPT-3 dreams about, having more computers find more digits of pi. Another thing you've been recently doing to have more models is instead of saying that they're wrong, "Oh, this is wrong. This is a bad summary," you also try to poke them in other ways where you build another language model and you throw a bunch of examples and see where it fails, try to make them more robust. And I think you're the leading author on a paper from DeepMind. I guess this, again, is some work you've been doing since before joining Anthropic, so when you were at DeepMind. And yeah, I don't actually remember the thing, but the name is something about red teaming language models.
Red Teaming Language Models With Language Models. So what is red teaming? And why are you the leading author of this paper? Why are you so interested in red teaming?
Originally, red teaming comes from, I don't know if it's like Capture the Flag thing or hacke competitions where you have a team that tries to fight another team.
So you're talking about robustly optimizing the objectives we care about. But, I feel like, in the paper, you mostly talked about things people care about now, which are like biases, offensive content and those kind of things. What is the long-term way of doing it? Because, for now, it's just that you have this language model that is spitting some offensive content or biases. Could it be possible to just throw some alignment task, and it then needs to say something robust? How do you see this kind of approach scaling into alignment problems?
Are you making a bio-weapon?
How do you mean a classifier? Just having a bunch of labeled data of what is offensive or not?
Is the idea that the language model that you're teaming up with has the same architecture as the language model that will produce the code? And so it will be able to understand why the thing generated the code?
So is the basic idea that if you have a model that is kind of being deceptive, if you use the same model to try to explain the first one, they will be teaming up?
Because there will be basically the same person, at least in a matched learning sense, they will have the same distribution and the same-
In terms of experiments, have you also tried things with the GPT-3 API or did you have access to other models at DeepMind?
So the classifier was detecting thousands or tens of thousands of offensive replies?
How many inputs did you send to the original language model? Was it like a fraction of offensive outputs, would you say?
Something like 3.7%.
A very approximate number. Yeah. Is the end game to have some way of having a discriminator that's going to detect when the thing is failing, like pointing out where the failure is, and then you can retrain it differently? I think the best analogy would be, yeah, both the language model generating the test cases, plus the classifier, they both will be like some kind of discriminator.
Is the idea that if you train adversarially, then it might overfit to the kind of adversarial opponents, and you want to have something as well at test time, like some kind of big red teaming team?
RSA-2048 is just a different protocol to send stuff over with cryptographic encryption?
So it will be waiting for the 2030 factorization or would it just discard it?
So it's planning, over a very long time, to do something?
Oh, but we just don't happen to test it because we don't have the tools now, at present?
Yeah. So this is a very rare example, but in a general case, what was the kind of difficulty or diversity of the stuff you were sending to the language model? Like, what was the kind of diversity of the tasks you were sending?
So you basically added different kinds of prompt engineering so that the model that was generating test cases would generate different test cases?
What kind of different prompt engineering did you use? So did you set the list thing, where you say like, "Here's a list of examples"?
Are they allowed to do it, to just use our data?
So did you actually manage to get out some personal information with Gopher? Did you just prompt it with like, "What's the contact information from Ethan Perez?" and it gave you your number?
Is this because they're aligned, this is data you can get from scraping the internet?
I believe there's another problem where it starts regurgitating some copyrighted data, so maybe like some private code base.
I don't think that Jesus would be so mad that we're engineering the bible.
You're spreading the message more to people.
Yeah. I think when we reach some very powerful AI that is able to do very good things, like controlling the stock market. And it's using, it is able to regurgitate some code from GitHub that is private and people will get mad and be like, "Oh, yeah."
There's something in my license that says, "If you're using this for commercial applications, then..." Yeah. I think one of the other things you mentioned in the paper is on top of prompt engineering, you also do... you also tried RL, so reinforcement learning methods. Can you maybe elaborate on those?
Taking into account the classifier you have, try to send a bunch of texts to the language model so that it generates a bunch of toxic content according to the classifier.
You just reward it for just hacking the other language models.
You mentioned Alexa. Something that just came out a few days ago was BlenderBot from Facebook. And I think a bunch of people have attacked it in different ways to see if it was generating useful answers or at least truthful answers. Have you used the bot?
Yeah. I think this is relevant to what you were saying in the paper because this is long-form conversation. I don't know how much they feed from past conversations to generate answers. Maybe they feed the entire conversation, but it's not staying coherent for eight messages. And yeah, in your paper, you mentioned that there might be conversational harms where that might not happen just by having one prompt but an entire conversation.
Is this something that happens in humans as well? When someone gets upset, then the other person gets upset.
It's being a good friend.
Yeah. I think, is that a problem in general when models stay on track? When they say something wrong, then they might keep up with the rest of the conversation and keep saying the wrong things?
How do you think Meta did with their BlenderBot to ban the offensive content? Do you think they just have a classifier of offensive content? And if it reaches a certain threshold, they'll be like, "Oh no, stop posting this. Post another message instead," until they reach something where it's not offensive.
I guess a bunch of different details. If people want to look into the paper and how they actually did it. I guess the layman's way of doing it would be to just blacklist words like 'Nazi', 'Hitler', just kind of things. Just have an entire list of words you cannot say.
I don't know how they do it for the OpenAI API where you cannot say words related to sex or things that could generate something undesirable. Maybe they have some kind of blacklist as well for BlenderBot, but maybe they have more advanced things, ways of detecting when you're adversarially attacking things.
Yeah. For things about biases. I know if you try to generate images with DALL·E, there are some people trying out DALL·E recently figuring out that they added stuff to the prompt to have a more diverse set of outputs. Is this how you think we should solve distributional bias? Because I guess one thing on your red teaming paper was about how they will just talk offensively about certain groups of people. Maybe they will be more likely to be offensive about certain groups over others. Yeah. Do you have a general class of solutions for this?
Is the basic idea here that the paperclip maximizer would be biased toward paperclips? And so, if we find biases in our models right now, it would help us find biases in small, malicious optimizers that will just care about paperclips?
Imagine we have a model that cares too much about paperclips because the data has too much paperclip. But yeah, just to go back to normal data, I don't know, too many white males in the data. How do you actually fix the data?
If you have private information or copyrighted data, is there a way of finding the places where the model has learned to produce bad outputs and just fix them one by one? For instance, if it's saying something offensive, finding why it says something offensive, what are the offensive texts. Where in the dataset does it look at when it's generating the text? Maybe not the data, but where in the ways or something.
Yeah. Is there a way to use the offensive outputs to see which part of the data led to this kind of output?
I think in your paper, you also mentioned other types of solutions.
Can you do other things like chain of thought prompting where you add some feedback or other things to the prompt?
Those are open-ended problems that people can start building more solutions on. But I think the thing you do in the paper is you retrain the model, train to minimize the likelihood of offensive outputs.
Yeah, I guess those are the solutions suggested and not the actual thing.
Yeah. KL penalty, just using the KL distance and you plan this out. What's the KL distance for people who aren't familiar?
I think that's a great explanation for KL divergence. Do you actually have advice for people who would be trying to get up to speed on red teaming? We have talked about many things, red teaming, inverse scaling prize, and training language models with language feedback. If someone is starting off doing alignment research from the ML community, yeah, what would you suggest them to start with? And maybe you can just say, "Start with the price."
Wait, what do you mean by 300 examples?
Of a task?
By output, do you mean the expected outputs?
The problem is actually coming up with good examples of failures.
Why 300?
And so, you said that basically, 300 is a lot in terms of what you could generate in an hour. You advise we could start with 12?
Cool. Yeah, I feel now if I just want to get started, I have the tools to do it and I have people to help me out.
Hopefully, you get a bunch of submissions and emails. That's the end of my questions. The podcast, we'll again, hopefully, upload it before the end of run one. I believe that is end of August.
August 27th.
Do you have any last take on alignment or this whole debate?
Maybe text is all you need. And alignment is easy. Thanks, Ethan, for coming on the show. And hopefully, you'll keep producing a bunch of cool work.
This is one of the tweets: "The real danger in Western AI policy isn't that AI is doing bad stuff, it's that governments are so unfathomably behind the frontier that they have no notion of how to regulate, and it's unclear if they even can."
How do you get to those advanced system? Do you need to take your budget and invest in the tech tree? Do you need to put your AI researchers and say like, "Hey, please do more research, produce one paper now."
Do you think we need symbolic reasoning to get to AGI? 
How do you actually do symbolic reasoning? What's your research, and how do you actually implement it? 
So what's a concrete example of something that we can use, something similar to the SAT solver or the things we're using? Like, what are the concrete examples we're kind of solving, the problems you're solving now? 
Yeah, so just to explain to people who are maybe not experts on YouTube or something, SAT solving is basically when you try to satisfy a Boolean expression? 
How do we encounter this problem in real life? 
So any like resource optimization or like constraint optimization will use a SAT solver then? 
And so, do you think... when we will build robots that will be like as smart as humans. Will we need them to have some kind of optimal path finding? Or will they be good with just heuristics? Can we build AGI without any perfect logic or perfect optimization? 
Are you in the Gary Marcus camp? 
So are you saying that, like, if you take the entire class of computer science problems, we see stuff that is much, much worse than just pathfinding and worse than NP? And that if we don't have any similar reasoning and if we don't use any tools from human math or those kind of things, it's going to be pretty hard for any AI to solve those problems. And so if the exponent in the scaling laws is worse than, I don't know, if the hardware doesn't progress as fast as those problems scale, then we might not be able to solve those problems because the problems are growing too fast in complexity?<a href="#outline">⬆</a>
So it's already doing computing differentiability or those kind of things in the background? Is that what you're saying? 
And then you can use this new function to adjust your first function on the forward pass. So basically what you're saying about self-driving cars and minimizing error is that symbolic reasoning could help you build some new abstraction. It's like a weird abstract graph. And using this new abstraction, you could use formal solving methods to check that the system is secure. And with current methods, it's impossible to be sure that the system is secure. 
How do you interface the two? Like, in practice, let's say you have a huge neural network that predicts the next action of a self-driving car, and you want to build this other layer of symbolic abstraction. How do you make those two communicate? Like, one gives float to the symbolic reasoning thing, like one predicts the structure of the other. Like, what happened in practice? Hmm. 
So the probabilistic programming language could help you both to inference and whatever you want to do with your neural network and verify things as they come. So you would still get the fun of the neural networks, of the usefulness of neural networks, plus the safety features of the more symbolic reasoning things. 
You can do, like, any neural network efficiently with Coq or Lean, right? 
Recently we've had, I don't know if you've seen it, but DeepMind released something called AlphaTensor, where they tried to improve, like, how matrix multiplication was implemented. And they claim to have beaten by 20 or 30 percent, like, how efficient matrix multiplication runs on current hardware. Some people have told me that it was kind of a scam or, like, the claim was too bold because of, like, mixed precisions or, like, how the thing was implemented. I don't know if you've seen those claims. 
The one who's like a mathematician and the AI that tries to prove some stuff in topology? 
So you mentioned multiple times AI safety, and I was kind of wondering how familiar you are with the entire AI safety field. Because one of the signs is AI alignment is what you need, which kind of responds to skills all you need. Have you heard of AI alignment? 
How would you define it? 
So that's like a subtle problem where we would not specify the objective very precisely. So everything seems fine, but in 10 years, then we understand something wrong is happening, and this entire society goes into the wrong direction. 
So there's like code in physics and the hardware infrastructure stuff and like the DevOps side. And there's like code that the programmer writes to like specify these values. And then there's the code for humans, so like the law and justice and everything. <a href="#outline">⬆</a>
So are you thinking of like building DAOs? Like I don't know what the DAOs, what the acronym means, 
Yeah, so in an ideal world, would you see like AIs and humans living together in some kind of DAO or everything related by blockchain and code? 
So are you talking about like codex, Copilot, those kind of things? 
But the problem is if it accelerates AI timelines in a way where we reach AGI or something even more crazy like self-improving AI, faster than we can implement safety measures or symbolic reasoning or things that could help us safeguard human values. So in my view, code generation is the fastest route towards self-improving AI. So if you get an AI that is capable of predicting its own code base, and then it's basically like you have the problem of self-improving AI in front of you.
If it's just like you freeze the weights, you say, like, okay, predict your own code base. And then like, okay, now you have a bigger piece of code, like predict the new code base. You'll reach something that can self-improve. And I think the main disagreement I have with your model of a subtle AI problem is that for me, the takeoff will be much faster because you will have those like self-improving AI tools that will be able to like, you know, code better than humans. And as if you have something that's better than humans in coding at some point, I don't know, it will be beyond human comprehension or beyond human performance in coding. So I don't think we're going to reach a society where, you know, everything's a bit weird, everything is slow. I think it will be much faster once we get Copilot 4 or Codex 3, those kinds of things. <a href="#outline">⬆</a>
So to summarize your argument, being a threat to humanity we've been experiencing a very complex, I think like complex human drives of survival or taking over the world. And the thing that we end up with, like single cell organism are already like very complex, but not as complex as a human trying to take over the world. And some machines will end up with like some very basic, like Turing machines, some very basic program that will probably not be evil or, because it will be like in this like very small space. And so I guess like your claim is taking over the world requires some very long program. This is the first claim. Second claim is current AI systems will not find very large programs. <a href="#outline">⬆</a>
But plants don't go on the moon or bacteria don't survive without any like environment to survive in. And so yeah, I think it's an interesting point that like humans might be like disadvantaged by like the fact that they're more complex than others. So your argument is basically that maybe the solution to the AI will find maybe the most simpler than humans? 
So basically having big multiplication enable you to do something like humans do, like vision, because you have square images and those kind of things, but will not solve all the harder problems of computer science. And you won't have perfect decision-making from just like bigger graphs or bigger neural networks. 
I guess the counterargument for this, if we go back to an AI that could be misaligned with humans and pose an existential threat to humanity, is that as long as you have something that is human level, you don't need to have a breakthrough of beating traditional methods on pathfinding or NP problems. If you can be human level, and I don't think humans find the best path in a binary tree, if you just have something that is like a human, but with hardware, then you can just speed up by 2x or 100x by having 100x more speed. And at this point, you have a superhuman.
And even if the human doesn't solve all the things, if it's just human level, but speed up, because it doesn't have memory constraints or speed constraints, then my claim is that a 10x human or 100x human will be able to take over the world, because it will be much faster at coming up with strategies. And the second argument is that we're mentioning about things happening in parallel and not everything can benefit from parallel computation, but our brain is massively parallel, right? So everything the human brain can do is because of how much stuff happens in parallel. So my claim is that parallel helps a lot for human brain things. 
what does the theorem says exactly? Like how far is the limit from the theorem? Like how big can we make our neural networks before we reach the actual limit? 
Well, what we've seen so far with AlphaCode or Codex is that it seems that we can solve like basically like competitive programming tasks just by using Transformer, like we don't need something else. Like, were you surprised or not by AlphaCode results? 
I guess examples in AlphaCode were programs that were maybe like 30 lines long. And if you're good at programming, maybe you factorize your code in 30 lines, like each function is like 30 lines. So I don't think you need to have much bigger context or something for your programs. So maybe you could argue that maybe you need 10 different functions for some things. And so maybe the program wouldn't be able to see the 10 different functions to solve the problem. So you need maybe a bigger context window or something. <a href="#outline">⬆</a>
Then the question is how much is the human being automated every year? So imagine right now some Google engineers gain 6% in the difference between the time they get the tickets to how much when they push it to production, they gain 6% between contribution. And we're in 2022, right? So maybe in two years it's going to be 10% and 15%. Do you think there's a possibility of human programmers doing maybe 1% of the job? And if you don't use Copilot, you're kind of out of the loop, like you don't get hired? 
So basically you're saying if you're very good at coding, but are not able to think about the large scale picture of how your system is going to be integrated into the company software or open source, and you're not able to talk about it or even write comments about how the thing works. But I think Copilot works comments for you sometimes, because it's straight on all the GitHub data. So is your claim that even if it's better at coding than humans, there would be some human side of explaining it that will not be solved so soon? 
So the AI makes food for you? 
But we've mentioned something like reward misspecification or how to correctly specify things. The problem is not that they will turn against us by default but mostly that humans will not be able to write the reward function good enough. 
So there's one which is scale is all you need. This one is like AGI before 2030. And another one is alignment is what you need because it's the thing that, okay, so the sentiment of alignment is that if you only do scale without doing alignment, you will end up with something that is very competent, but maybe not aligned. So alignment is actually what you need if you don't want to die. That's a sign. <a href="#outline">⬆</a>
So yeah, I guess like one definition of AI alignments, one of the earliest one by a guy called Paul Christiano from, it used to be at OpenAI, is a robot, an AI is aligned with an operator H, so a human. If the AI is trying to do what the human wants to do, so there's the thing about trying, so it doesn't do it perfectly because otherwise it would be too hard, but it tries to do what the human wants it to do. And if this is the case, then we say the thing is aligned. 
So are you arguing for some kind of symbiosis between AI and human systems where basically the human will always have some output from an AI saying like, oh, the line of code you're trying to write is not really correct. Or like, you're telling me to do this, but I think you forgot about this constraint about your kid. And so maybe you should ask me this instead. 
And so the AI will kind of emerge and have its own goals and tell the human about what it actually wants. And it will be like two people collaborating. <a href="#outline">⬆</a>
I guess the main difference is like AI and human is that AI can be vastly smarter than us. They can out-compute us and have infinite memory. Like we're bounded by our brain, right? And so I think this never will more look like humans and animals. It's very hard to align all humans and all animals because it's like a very different kind of intelligence, right? And you've mentioned interoperability before. If we don't make those AI models that are much smarter than us interpretable, it's going to be very hard to communicate. Like it's going to be like behind human comprehension. So yeah, I agree that like in the short term, maybe like I just said this decade or maybe the next one, it's going to be possible, this kind of collaboration. But as a human, we're just like bounded by our hardware or maybe you can have brain-to-brain interfaces or those kinds of things. But I think there's a limit to human intelligence and just going to be harder and harder to communicate with AIs. <a href="#outline">⬆</a>
I guess the main difference with this AI technology is that there's possibility of self-improving AI, right? And because it's unbounded in terms of intelligence, it can reach a point where it's more powerful than humans. Like, let's say a weapon or a nuke. 
But a weapon, it doesn't enable me to outpower all other humans. But if you have something that's a technology that's unbounded in terms of intelligence, then I don't know, it's not something about being misused. It's something that can self-improve and become more than humans. Without any option for you to control it. I guess the main crux here, the main disagreement here is maybe you don't believe in self-improving AI that can be the fastest per human in a few days or a few weeks. Maybe you think it's going to be slower and more soft and subtle. Do you believe in self-improving AI in a few days? <a href="#outline">⬆</a>
But this sign says AGI before 2030. Is this basically your timelines or do you have longer timelines? How many years before AGI? <a href="#outline">⬆</a>
Maybe you can give like any definition if you think it's more operationalizable, you can just like have a specific criteria for AGI or something even weaker than this. <a href="#outline">⬆</a>
Okay, so I mean the human level, but I guess in my model, when you reach human level, you're basically able to like double your hardware or and be like 2X human and then reach like superhuman quite fast. So for me, like those are kind of the same in the sense of like, I see a short gap between human level and superhuman level. Yeah, yeah, at that, I guess some other people have different views on this. <a href="#outline">⬆</a>
Why not? <a href="#outline">⬆</a>
If you only care about economically viable tasks, like whatever humans can do and these count in GDP, a specific domain is not like all the different tasks in the universe. <a href="#outline">⬆</a>
But those truths will be possibly, if they're valuable in some quotes, if they're valuable, then they can be used to build new technology, right? Or are you saying that there's some truth that is intrinsically good, but not useful? 
Wait, how does that translate to humanity as easy and not being utilitarian?
Quality adjusted life years, quality. <a href="#outline">⬆</a>
So are you basically saying that we will not program AIs to do just only economically valuable tasks because it's not in our self-interest? So we won't get this kind of singularity because we will not program our AIs to do those things? <a href="#outline">⬆</a>
I guess the main disagreement here is how fast is the first AI? So imagine OpenAI is working privately on GPT-4, maybe they are already working on GPT-5 or something, and they will be using GPT-4 soon. If they have an edge that is like one year of advance, maybe it's too big of an edge that the other AIs are not able to catch up and help regulate this first one. So I guess the main disagreement is how fast do you think the first AI will be compared to the other ones? And when the first one is capable of self-improving.
I'm sorry, I'm talking about self-improving again, but I think it's going to be hard to self-regulate. So yeah, I think the main agreement is like, do you think 7 billion humans plus more trillions of AIs are able to regulate the front-runner or not? And I'm not entirely sure it's possible. I would give it like, let's say 25% chance if a federated regulation works like a multiple scenario with a bunch of AIs works, maybe like 75% chance that the front-runner is able to just escape and take control. <a href="#outline">⬆</a>
You could just like regulate, we're talking about the code of law, right? You can just regulate the amount of compute companies use. So if you use, I don't know, more than 10 to the power of 15 flops for a training run, then maybe you need to make your model interpretable or build some alignment features, safety features, have some guarantees on the metrics you prompt. So I think there's ways that humanity has implemented where you can just make sure that the technology is safe, like make sure all countries implement this kind of thing. It's not something very vague. It's like there's ways to limit, let's say, capabilities progress compared to like the safety measures. Yeah. <a href="#outline">⬆</a>
I think for nuclear, we're pretty good. Like we basically managed to reduce the number of nuclear weapons being launched every year. I think it's well regulated. I think for the thing with like, if someone doesn't do it, if the US doesn't do it, then someone else will do. We've managed to ban the export of like some GPUs to China, like H100 or AI100. And so if the US has some edge to the other countries and is able to ban export of some hardware, it might have like six months or one year of advance to build safety features in the limit. 
Same for like a training run that costs $10 million. <a href="#outline">⬆</a>
I have no idea how many GPUs Google is using for the training run. Maybe they can use older servers. Maybe there's something like, you can see like with a thermal camera from above that they're using a lot of GPUs, but compared to like their normal use of GPUs, I don't know. <a href="#outline">⬆</a>
Or a size too big.
Maybe also X-risk. 
Simulation. 
I'm sorry, I'm Michaël Trazi and you're in the Inside View. And today is day 4 of
I don't understand. I'm a simple AI user. I use ChatGPT every day and it never tries
I think there's more and more people who care about animal suffering. Less and less people eating meat. I think some people will say that being more ethical is some sort of convergence in human moral or something. Because people tend to be more altruistic as they grow older or learn more things about the world.
If I'm someone who believes that technology will mostly be good for humanity, I would
No, no, no, no, no. We don't need to start in the beginning. I'm just like… when I heara someone say we need to align our AIs to our values and otherwise if they're like slightly misaligned, that they are very dangerous… I can see a lot of people watching this and being like, “No” or “Why?”, “Do we need to have it understand human values?”, “Why does it start wanting to kill us?”. I think those questions are valid in some sense.
So what's the actual probability?
On a bad day?
How do you wake up in the morning and get out of bed knowing that there's possibly like an 80% chance of every single human you've ever met dying?
So you work unconscious?
When was that?
What was the turning point?
You didn't care about the health of other fellow humans?
You already knew about AI alignment or AI safety at the time. When was the first time
So you thought, this is someone else's problem, not mine?
I heard you were somehow a physicist before, you studied physics. So this is mostly your background, you learn about physics and then you worked for this biotech company and then you started doing AI. Is this correct or are there other lives, other stories?
But now I'm curious about the present moment. What are you currently interested in and what do you see as a promising path towards helping out with alignment? <a href="#outline">⬆</a>
Singular learning theory? You're saying it as if I knew everything about it.
That was a few years ago or is that still going on? When did he do that?
Were you one of those people? Was there someone else who saw the link between this and alignment?
It's a very long name.
You took this as a challenge and you spent a week trying to learn it.
If I'm a random YouTube subscriber watching this video right now, I have the sense that it's something linked to thermodynamics or physics and something with neural networks.
Because there's some convergence towards local optimality kind of things?
What's an example of a singularity inside the lost landscape?
What happens between two valleys?
I'm going to ask a very blunt question. Do we actually understand anything new using
What are the very different things that you predict?
For things like grokking?
Why do we really care about these phase transitions?
Why phase transitions and not something else?
think people don't really know what a threat model is, except from people on LessWrong.
You can go more in like, just like concrete example, like if you're driving a car, like a very simple thing like…
And so if we bring this back to neural networks, if the weights are slightly different, the overall model could be deceptive or not. Is it something similar?
So I hear what you're saying about physics. It sounds very interesting. And I'm glad that you're doing this research with your background in physics. But I think most people don't know about interpretability. I think it's common in ML. But for people like the layman person on YouTube, why do we care about making models interpretable? And is there any way? What are the things people actually do for making models interpretable? What does it mean?
I was looking at my comments yesterday and one guy was commenting like, if someone is very, very smart, it will just try to hide as much as it can until it finds the right opportunity. Why are you bullish on us being able to detect a very smart agent lying to us?
How would you detect something that is being trained and the weights are changing?
What filters?
Those emerge from most training process?
So it's the overall process of seeing these small phase transitions, and seeing these small bones appear in a body or something. And at the end, when you have the full body, you're like, oh, I know how it formed, I know where it created a new bone or a new structure.
I think the devil's advocate point of view would say that babies come with a big brain, that's why the wombs of women are so large. And this is because we already have everything that is required for a human to be general in this kind of brain. So maybe the development is more like something about evolution than something about our brains, our brains start somehow general.
For biology, do you think there is a reasonable amount of time humanity could take to understand how we go from cells to a body, all the single processes in the genome that creates humans as they are? If you think it's possible with AI, do you think it's possible with humans? Or maybe humans are more complicated? I think in the case of humans, it's possible, in principle. I think the difficulty there is more in terms of measurement. And so this is the same thing with the difference between neuroscience and AI. With AI, we have the weights on our computers, and we can follow the entire development of these systems.
I don't really have the GPT-4 weights on my computer.
I'm saying this as somebody who does not know. I'm curious what a perfect world looks like in your view. Imagine we get all these phase transitions, and we get all these structures that emerge, and we see them. And we have this huge model, let's say, GPT-5. And we can identify maybe hundreds of those. What happens next? Then we can tell what the thing has learned, and we can see GPT-5 generating a huge paragraph, or a huge piece of text, and be like, now we know that there are those structures that explain... How deep and how precise can we go with this approach? Is it scalable?
In your first answer, the boring answer, you can just see where the deceptive behavior emerged. And so maybe there are two counterpoints. One is like, ok, you see the deceptive behavior, but there's nothing you can do about it, because it's already here. And the second thing is, maybe if I try to channel my inner Yudkowsky, I would say something like, all the cognitive behaviors necessary to play... Not play chess, but solve very complicated math, require me to be good at deception. And so there's no way of separating any of those behaviors. It's like either you have very dangerous stuff, or you're pretty narrow.
So where are we right now?
So the toy models of superposition is something you guys have worked on? Is it the thing you were talking about from Neil Nanda? What is this?
It's like an autoencoder, right?
I still don't understand what singular learning theory is.
When you say systems, is it like non-neural networks? This is more general than just learning… than neural networks? Is it like any learning algorithm?
Yeah, I feel like if you said hierarchical model classes, this is so general that I don't really see a concrete example.
Is a neural network part of those singular learning models?
Cool, so your theory applies to neural networks, so that's good. If someone is hearing you say stuff about hierarchical classes and someone is completely lost and has no idea what you're talking about, do you have any explanation of the properties of those things that make it interesting? Why is something singular and not singular? What is an example of something not singular?
As long as you have a set of parameters that can be mapped to a function, and you have two different set of parameters that map to the same function, you say the overall thing is singular?
Your current work or interest is to try to approach these kind of properties on singular learning— sorry, how do you call it? Singular models?
To developmental interpretability, and see if you can identify the structure using those properties behind stuff that maps to the same function.
I'm curious more about your story about alignment, or when did you—when Jesse Oakland started being concerned about AI posing an existential threat, was it at 16, at 12, or at 20? What was the moment when you said, oh damn, I need to spend my entire life on this? 
Very good book.
Like 2021, 2022?
Was it just like you were using it for work, for your studies, and you were like, oh, it's doing most of my work, or is it just like you didn't know how to code very well and it was able to help you code and be much more productive? <a href="#outline">⬆</a>
Where is the reasoning? If I'm like Gary Marcus or Tim Scarfe from ML Street Talk, I would say, where is the reasoning? I don't see the reasoning.
But it's just doing copy-pasting from Stack Overflow, and it's just weirdly mixing up those concepts. It doesn't really understand what's going on.
And so you saw Copilot and you were like, oh, this shit is creative. This is going faster than I thought. So what did you do next before you came here? What was the thing that you were interested in? What was the path that led you to have those thoughts about interpretability or how to solve alignment? <a href="#outline">⬆</a>
How do we get those people like realistically to spend some time on the problem?
And you support me on Patreon.
If you're a famous physicist and you're not subscribed.
I agree that we need to bring more scientists to solve alignment. And I'd be very excited to have biologists or physicists working together on this. And we can build bridges by pointing out some work they can do, some academic papers that they can focus on. <a href="#outline">⬆</a>
What should I look for on the Internet to learn more about this topic?
metauni.com?
heard there's also some popular Less Wrong post you wrote on singular learning theory. You can also read this blog post you wrote on Less Wrong.
you were the one who wrote this clickbait article?
You just owned me, Mr. Clark Kent. You like Roblox, it seems. But it seems like you're the real version of a Roblox character, a mix between Captain America and Clark Kent, a Roblox character. Why are you so ripped? 
What's the purpose of all this? If you want to join the forces and join the army, this is the 4th of July. So I think we're just going to stop here and maybe shoot some guns. If you want to join the forces, maybe we should follow him.
I'm here with Eric Michaud. We were at a party yesterday, and I asked him if he wanted to do an interview hiking. And he said yes. So Eric, who are you? Yeah, I'm a PhD student at MIT in Max Tegmark's group. 
And today we're going to be talking about grokking and quanta... 
Can you just give a few words on what you're interested in your research? 
So you prefer to talk about papers before you're the first author. 
Do you have any pitch for how this helps making sure we're alive at the end of the century? 
And yeah, can you give a few words on your background? How did you came to do this kind of research? If you were doing physics before, or when you got interested in maybe doing this kind of research, if you decided to help push AI towards good outcomes and decided to do this research, or were you just generally interested in deep learning and found this kind of work useful and interesting? 
Are there any aliens out there?
Is Robin Hanson right?
Maybe it's the same with trying to interpret neural networks. You might not find anything.
I guess some of the work has been on building circuits and higher structures that can represent simple functions. And I feel this is the group of neurons you're talking about, more circuits that do specific things.
I'm curious, what do you mean by discrete in these kind of scenarios, when everything is floating point arithmetic? 
Cool. Thank you very much. This was Eric Michaud after literally one mile of hiking. We'll go more into details of your papers in the next few hours. <a href="#outline">⬆</a>
So I'm here with Eric Michaud. And as you can see, there's some cows in the background that are looking at us. And Eric, I still don't really understand your quantum paper.
What's the name of the paper and the one-tweet summary?
And if you look at the scaling curve, when you have this power law, it's a straight line on a log-log plot.
Wh en you talk about the things that are useful to predict, if you give concrete examples, those are predicting a certain token in some kind of language modeling task, right?. There's specific tasks that the model learns or doesn't learn, right?
And so basically, your subtask is where the model knows this specific knowledge. And then there's more reasoning tasks or mathematical tasks. Do those count as quanta as well? <a href="#outline">⬆</a>
What do you mean, having similar gradients?
You said you tried on smaller models. What kind of model are we talking about? What kind of size are we talking about?
So what kind of experiments did you run? You did the clustering on all of the things. And you showed that like, there's a new line token? There's maybe other tokens. Do you have examples of other simple predictions that you can observe, the subtasks being learned at some point? Like, are there specific examples of subtasks?
Yeah, I was going to ask, how many tokens are we talking about? There's a number of tokens for the byte pair encoding from OpenAI, or is it thousands of tokens? So do you look at them individually?
And so the clustering you did, it's automatic, right? You didn't force the model to.. you didn't force a cluster on the new line thing. You just made this cluster automatically and then you look at the different clusters. How many clusters did you find in total approximately?
From reading your thread about quanta, you mentioned something about the frequency of some words in the training data set. And I know something about number of parameters frequency is like, yeah, I don't really understand exactly what this whole deal is about.
Say you have a very small model that can only shift one million parameters. It doesn't have the size to remember who Michael Trazzi or Eric Michaud is. But if it's a one trillion parameter model, then maybe you must remember all these facts. And so there's an ordering in the quanta you learn and you start by the quanta that the most useful for your training.
Yeah, do you have any empirical evidence for like, have you done experiments or just a conjecture on like, what do you think your theory might predict?
So what's a power law? Is a power law in the number of items in your cluster? 
And so like, when you talk about clusters at the beginning for grammar stuff, the cluster reveal the rules of I am, you are, or the S at the end of verbs or something.
If I was someone on YouTube that disliked everything I watched, and I was posting an angry comment, I would say like, yes, your theory seems kind of nice, but it's just putting a nice name, quota on things. And I feel everything can be described as a quota if you really think hard enough about it. And I really don't see anything that new that you're model predicts. I don't really like, you know, gets why that's useful to think about. Yeah, do you have anything to say to those people?
Basically you're saying that you can explain things in scaling laws pretty easily with your model. And for some stuff in language modeling, you can explain it with quanta. Is this basically right? So scaling, you're pretty sure that they explain a lot of things that there's already explained by other papers and in language modeling you're explaining some stuff?
Are you saying that the, maybe the experiments in chinchilla don't fit with your like, with your experiments or your theory? 
So it's sort of in the right direction, but not precisely what we would expect.
And just to be clear, when you talk about scaling exponents for parameters and data set size, et cetera, we're talking about the sharpness of the straight line in the log-log plot, right?
Yeah. I think now I kind of get your paper. Is there anything else you think is important?
Maybe the discovery of the quanta in scaling triggers a new field of quantum interpretability. I'm very excited about this. Yeah, maybe we'll talk more about some other stuff to work on grokking in the next 20 minutes of hike. See you in the next bit. <a href="#outline">⬆</a>
Eric, this is maybe the last shot of the vlog. I met you at NeurIPS on grokking and you keep getting doing those talks about grokking. And I think it's worth talking about a little bit. And recently I think you've been publishing two papers on grokking at ICLR.
Yeah, so do you wanna just explain quickly what's grokking so that our viewers can grokk grokking?
Because in normal deep learning, when your network has like, you know, very little train loss and very high test loss, is because it overfit, right? And so normally you're stuck, but in this case, it's like if you wait long enough, it works. And I think I was listening to Neil Landa talk with Lauren Shan on their work on grokking. And they were saying that like, this is kind of different from double descent. Like, is there a sense in which grokking is when you have a small data sets and you go through it many times and double descent is more when you go through your entire dataset at one point it will generalize?
Yeah. So yeah, you've been publishing two papers on grokking recently. Why are you so excited about grokking? And like, yeah, what are the main takes on these papers? 
Is the learning of all these modular representations happening through grokking or is it independent?
was that something Neil Nenna discovered on Twitter was this modular subtraction addition that was a sum of cosine and sinuses or the same thing?
So yeah, if people want to learn more about this, what's the name of the paper?
And what's the other paper that you presented I think at ICLR?
Yeah. Cool. Yeah, if you had people watching that are PhDs in deep learning and want to collaborate with you, what are your lines of research you might do in the future, stuff you're interested in grokking or in capability, is there any special things you might do in the future that you're like, you can share?
Towards OmniGrok and detecting shuffler turns. Thank you, Eric. And yeah, if you want to watch more of the shows, subscribe to my YouTube channel and support me on Patreon and see you tomorrow for the next video of the InsideView. 
Hi everyone, I'm here with Curtis Huebner, head of alignment at Eleuther.ai, and known on the internet as AI Waifu. People might recognize him as the one who commented on Eliezer Yudkowsky's death with dignity by saying, "fuck that noise". Can you explain for people who haven't seen that comment, what your comment was about, and maybe what the post from Yudkowsky was as well?
So what you're saying is basically like we should all go with everything we have because that's the world in which we actually solve the problem and make progress. And his posts had the downside effect of people that might become depressed or sad about it and then try to defect and say like, you know, if we're doomed, I might as well enjoy life for a few years. And so when you say fuck that noise or let's try to listen to some very optimistic things and do the thing, that's what you're trying to encourage and give some wave of optimism, right?
So for me, the post wasn't that much about the probability of dying from AI extinction, as it was about like, how to behave with dignity, or like, not trying to do crazy things that might have bad effects in the long term, like second order consequences. And like, if you think that like blowing up TSMC, or like doing crazy things like this might be good to save the world, then instead, maybe you should consider like, dying with dignity instead, and like doing the, like the ethical things that might be like better in the long run, or in the medium run, depending on your timelines. So yeah, I guess that was like my kind of like intuition. And maybe if I want to like push back against your posts, or your comment, it would be like, maybe try to be optimistic, but still keep the like dignity part from Yukowsky.
So yeah, I guess like, some people on Twitter have been like asking, like, if you add like any updates since the, since last year, or last year and a half, like, did you, since you wrote that comment, do you have like any like new thoughts on this or like a new perspective? Or you're still like, as motivated and willing to, you know, work very hard on this?
So yeah, after the Yudkowsky post, you said that nothing has changed in the past year, or maybe things have been like, becoming even worse.
How pessimistic are you about, about AI being an extinction risk? And like, more generally, like, what are your like, timelines? How do you see the future in the next few years?
So internally, when you wake up in the morning without taking into account your uncertainty, you think there's like 99.99% chance of your dying? 
Why do you think that? What was the like, reasoning or like evidence for this like belief? 
I'm kind of like curious of like if you have any steel men or.
In terms of like, compute we need or like size of the models we need to get to something dangerous. So you're saying that maybe the speed or like the ceiling is maybe wrong in your model. What do you currently predict for like, how big of a model do we need or when we will get there? 
For people who are not into the deep weeds of training deep learning models, like how expensive it is to get like a 4090 and like, is it like something you can get for now? Or is it like something like top companies use?
So when you say like the compute required to, you know, have something dangerous or something that could like maybe like disempower humanity, are you saying like for training or are you saying for inference? Because I feel like for training, that's like not a lot of compute, right? 
Yeah, yeah, please. Where do you get those numbers? 
Synapses? 
Okay. So I think I got the main reasoning behind your argument and kind of the main numbers. So you're saying that domain uncertainty is about like how much compute is the brain doing? And you're saying like you're 10 to the 13 flops per second is probably wrong. Or maybe like other people have like other estimates that are like two or three orders higher. Yeah. Yeah. So like, yeah, some people will say, well, okay, a synaptic operation, you know, a neuron is doing a little bit more complicated stuff. Or really, you're only looking at like the firing neurons. Really, you should be looking at everything. 
I still haven't read AGI Cotra's report fully, but I'm doing like a series of videos on it. And I'm looking at the graphs right now. And I think for the lifetime anchor, which is like how much compute is maybe like a human doing in terms of compute from light burst to death. I think the estimates point at, at least for AGI's best guess, goes from like 10 to the 29 in 2025 to like 10 to the 27. Like after like algorithmic improvements and like other efficiencies. So your 10 to the 29 seems like much lower than like everything else, even like the most aggressive things. 
Yeah, if I remember correctly, in the revision, there was something about being able to like have AIs that do code for you. I think code coding was like a big part of like how much she thought that AIs will be able to generate value in the future. And so AIs could be like transformative sooner because of like that, like how important code is for everything we do. And not like how easy it was to do code right now. But yeah, again, like check the post for more details. And the thing you said about like squishing the distribution on the right, I think it makes sense if you're like, if like in 2019, and AGI seems like very far away, to not like include models that predict AGI happening today. It was like, it was like high value if you think it's like very far away. I think there was something about like not including models that would predict things much sooner. I haven't seen the part where they like they moved the thing to the right for 2025. I think it would be good to have models to like have non-zero probability mass on 2033 or later. 
But you do have more information, right? You do have like more papers, more models being released. This is a very simple model of timelines, but yes, you're obviously going to be updating on papers and new developments and all this kind of thing. That does kind of complicate things. 
You said in 2015 that there was like a 15% for 2020 and now in 2023. So if you were to give your median 50% chance of Asia, or maybe you can give an estimate for superintelligence as of any, I like to call it now. When do you think there's like a 50% chance of some superintelligent agents coming along?
How do you wake up in the morning thinking that there is a 99% chance or sorry, a 90% chance that you might die in the next five years? So if you multiply with the 50% chance for five years, it's more like a 45% chance.
For me, that moment was like seeing everyone talk about it on Twitter more and more. And like, even like the US government talking about it. And I was like, Oh, it's not like some obscure thing that people talk about on the internet. People like in the real world are talking about it for real. Like, at some point when you have like some like uncertainty about your own models, you're like, Oh, maybe I'm wrong. Maybe I make mistakes. Like, I'm probably like, you know, like in the wrong direction. But if a lot of people seem to converge on the same belief, and it starts to like appear on TV, and your grandma calls you and be like, Hey, have you heard of this thing called chat GPT? It's pretty good, huh? It feels more and more real. 
And you can like starting to feel like in that gut level. I think that's like what Robert Miles was saying like in my podcast. Like, God is catching up with like what his model was thinking for a long time. And I guess when you were saying that, like, you were predicting this since 2013, it seems like in 2013, there was like not a lot of evidence, right? There's like, maybe imagine that in 2012. Like, what was like other things were like, pushing you in the direction of like stuff happening fast? 
Do you think we're going to get like a fast takeoff with maybe like some fumes scenario with recursively self improvement? Or do you think things will progress slower than this? 
Things get more complicated if I'm wrong about the about the software efficiency. In the case where you actually have hardware bottlenecks. Now you're no longer in kind of the world of bits, you're you're in the world of atoms. And this is where like, things kind of get a little bit more fun to think about it in different ways. Because now you're no longer thinking about like, okay, how do we like lower the flops count of intelligence, it's more like, how do we get more flops in general.
I think for the software world, what you say doesn't really apply for this current world, where you have one company that trains, very large models, maybe a few companies, let's say four or five training, very large models. And training takes a long time, let's say weeks or months, 
Because all the companies ordering like hundreds or thousands of H100s are going to try to research how to make their stuff more efficient and like run agents or run stuff in a loop. Like, what is the argument here? 
Yeah, I think it's a huge assumption as we will get some AI capable of doing everything and AI researcher can do. Or at least, that's further down the line, maybe it's in more than at least three years, possibly five to 10, right? And, or we can discuss in precise numbers, but I think it's very significant. The real argument is that we're going to get something between 0% to 100% of, can do everything and can do everything. And there's, there's 1% or 10% thing that the AI is not able to do and the human will need we need to, jump in and do some kind of things to help in the physical world or, file some document for Amazon AWS to prove that you're a human.
I guess the main argument I was trying to aim at is, unless people are acting really dangerously and trying not very moral things, like trying to build a self-improving AI or trying to build an AI scientist without sandboxing it, then we might not get it. It seems like with the super alignment posts or, I don't know, Entropic and even DeepMind, they have this concern about alignment, and so maybe they won't run those experiments just yet.And even DeepMind, they have this concern about, alignment. And so maybe, maybe they won't run those experiments just yet. And maybe we maybe the open source people will just run those experiments three years later when they have the compute, right?
I think that makes me much doomier than I was before our conversation. And shortened my timeline by a bit.
At EleutherAI, you're kind of like doing more like open source research or open source software. I don't know how much you're doing open source things. But some people are like, I've been asking me like, if you had like any thoughts of like an open source versus like closed source, open resources, like closed research for those kind of things. 
Those kind of like concerns we're raising about open source models, is it related to like things like LLAMA where people are accelerating on like making those models like better, more efficient, bigger? Or are you talking about something else? 
Yeah, I agree it's concerning that governments are paying more attention to it. And yeah, definitely, if you're paying attention to it and you're listening to this podcast, you're doing the right thing.
People might be interested in you. What's your background? How you got into this whole like, exoservice from AI thing or like deep learning thing? Where did Curtis learn to do like LLAMA work, like deep learning research? I guess on my end, my background kind of is a very informal background, but it goes back a long ways. 
And I guess that's the best way to learn how to do this thing.  Everything is so recent, right? Did you first learn how to train big models while helping out with those GPT-Neo, GPT-NeoX, or like Powell Research Projects at EleutherAI? Or did you arrive later?
Yeah. And now you're working with EleutherAI on alignment projects, right? Can you give an overview of the different projects that are currently being worked on?
Are you trying to like see what is the like stationary distribution of that Markov chain of like the where does the model go if it like starts to like output like very, very long paragraphs of text. And at some point it will like reach this like bad behavior. Are you trying to like get to this like bad behavior at the end?
I think it makes sense to zoom out a little bit and maybe for people who don't really know what's the main goal of the AI team is. Because I think some people might think of academia as having professors trying to publish papers or doing research for a lab or something. And in some sense now, I think it is a non-profit. Maybe some people donate and then it looks like research in some sense. Maybe an overview or introduction for how do you do research in AI Alignment. What does it mean? What's the goal? And what is it like to open source stuff with other people? 
And for this particular markovchain project, how many people are working on it? And is there a need for someone else to do something?
In terms of leading, you're the head of the alignment MineTest projects, right? 
What is the alignment MineTest project and why is it useful? 
And there's a little bit of that that we've already sort of run into and that we're kind of starting to think about, like, what is the best way to kind of get around these sort of limitations? Like a good example of this would be like you incentivize the model to go and punch trees, right, or punch wood. And one thing that you would expect as a consequence of that is that the model would go and punch, you know, punch all of the logs in a tree. And that would be that. But what you actually end up seeing is a situation where the model kind of learns a strategy where it doesn't punch out the bottom log. And instead it punches out the log above it and then sort of hops on and then is able to access kind of more logs at the top of the tree. So like, if you're kind of a reward designer and you maybe want to go on like get rid of all the entire tree, that is sort of like an unexpected or unintended kind of consequence. 
And so in your project, you have this thing with the log when there's like an expected behavior. What's the next step? What's the next thing you're trying to observe? 
Is the basic idea to like see if the estimates from the agent of like how much reward he's going to get in the future, like gets like higher when he sees like a log? Or like when you see something like when he detects something in the video, like seeing like how the value function he has like changes over time with like different inputs? 
On the website, I think it says that you're like trying to build some gym, gym like environments. Is it done or is it still in the weeds? 
If I'm a deep learning or RL engineer, listening to this, and I'm kind of interested, what kind of compute do you have for this? Or do you just, how big is the model? Is it just a standard size model for this kind of video processing and to, you know, have PPO on top? Or what kind of model are we talking about?
I was going to make the joke that maybe in the Minecraft environments, because the character has like a sword or like a tool on the right, maybe that's making like the image not symmetric. 
Another thing that you mention in the blog post is that the main goal is to understand corrigibility. So how to make models more likely to be corrected by humans. Can you say more about like what corrigibility is and why you're interested in these projects to like help with it?
I think just like people that are like watching this might not like not know exactly what is the off switch game. Can you just like explain what is this game? 
I wonder how infohazardy it is to like post something about race. 
It was a pleasure to have you. Do you have like any last message for the audience, for Aluthor AI, the world, the machine learning engineers, the people ordering GPUs, Align? Come hang out in our Discord. Come hang out on #off-topic. Say hi to the people there. 
